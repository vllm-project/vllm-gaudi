model_text:
  MODELS:
    - deepseek-ai/DeepSeek-R1-Distill-Llama-70B
    - meta-llama/Llama-3.1-70B-Instruct
    - meta-llama/Llama-3.1-405B-Instruct
    - meta-llama/Llama-3.1-8B-Instruct
    - meta-llama/Llama-3.2-1B-Instruct
    - meta-llama/Llama-3.2-3B-Instruct
    - meta-llama/Llama-3.3-70B-Instruct
    - mistralai/Mistral-7B-Instruct-v0.2
    - mistralai/Mixtral-8x22B-Instruct-v0.1
    - mistralai/Mixtral-8x7B-Instruct-v0.1
    - Qwen/Qwen2.5-14B-Instruct
    - Qwen/Qwen2.5-32B-Instruct
    - Qwen/Qwen2.5-72B-Instruct
    - Qwen/Qwen2.5-7B-Instruct
    - ibm-granite/granite-8b-code-instruct-4k
    - ibm-granite/granite-20b-code-instruct-8k
  DATASET: /workspace/vllm/benchmarks/sonnet.txt
  DATASET_NAME: sonnet
  ENDPOINT: /v1/completions
  BACKEND: vllm
  INPUT_TOK: 2048
  OUTPUT_TOK: 2048
  CONCURRENT_REQ: 64
  NUM_PROMPTS: 640
  MAX_MODEL_LEN: 4352
  PREFIX_LEN: 100

model_vision:
  MODELS:
    - meta-llama/Llama-3.2-11B-Vision-Instruct
    - meta-llama/Llama-3.2-90B-Vision-Instruct
    - Qwen/Qwen2.5-VL-7B-Instruct
  DATASET: lmarena-ai/vision-arena-bench-v0.1
  DATASET_NAME: hf
  BACKEND: openai-chat
  ENDPOINT: /v1/chat/completions
  CONCURRENT_REQ: 64
  NUM_PROMPTS: 500