diff --git a/vllm_gaudi/extension/unified.py b/vllm_gaudi/extension/unified.py
index 4f1bce8..24dcfab 100644
--- a/vllm_gaudi/extension/unified.py
+++ b/vllm_gaudi/extension/unified.py
@@ -102,21 +102,23 @@ def optional(op):
     return opt_impl
 
 
-def get_vecsize_packsize(dtype: torch.dtype) -> tuple[int, float]:
+def get_vecsize_packsize(dtype: torch.dtype) -> tuple[int, int]:
     """Get vecsize and packsize for given dtype"""
-    pack_size = 8.0
+    pack_size = 8
     if hpu_ops.is_hpu_gaudi3:
         return 128 if dtype == torch.bfloat16 else 64, pack_size
     return 1, pack_size
 
 
-def create_softmax_fa2_input_tensors(attn: torch.tensor, fmin: torch.tensor) -> tuple[torch.tensor, torch.tensor]:
+def create_softmax_fa2_input_tensors(attn: torch.tensor, fmin: torch.tensor, zero: torch.tensor) -> tuple[torch.tensor, torch.tensor]:
     """Create dummy input tensors for the softmax_fa2 operation."""
     vec_size, pack_size = get_vecsize_packsize(attn.dtype)
     retained_shape = list(attn.shape[:-1])
     retained_shape[-1] = math.ceil(float(retained_shape[-1]) / pack_size) * vec_size
+    # inputM_hpu = fmin.clone().view(()).clone().expand(retained_shape)
     inputM_hpu = torch.ones(retained_shape, dtype=attn.dtype, device="hpu") * fmin
     inputL_hpu = torch.zeros(retained_shape, dtype=attn.dtype, device="hpu")
+    # inputL_hpu = zero.clone().view(()).expand(retained_shape)
     return inputM_hpu, inputL_hpu
 
 
@@ -127,7 +129,7 @@ def convert_cl_aligned_tensor(input_hpu, reference_size) -> torch.tensor:
     input_hpu_shape[-1] = -1
     input_hpu_shape.append(vec_size)
     input_hpu = input_hpu.reshape(input_hpu_shape)
-    input_hpu = input_hpu[..., :int(pack_size)]
+    input_hpu = input_hpu[..., :pack_size]
     input_hpu = torch.flatten(input_hpu, start_dim=-2, end_dim=-1)
     input_hpu = input_hpu[..., :reference_size[-1]]
     return input_hpu
@@ -149,7 +151,7 @@ def merge(*attn_results: torch.tensor, feps: torch.tensor) -> torch.tensor:
 
 
 def partial_attn_causal(query: torch.tensor, key: torch.tensor, value: torch.tensor, bias: Optional[torch.tensor],
-                        slice_size: int, fmin: torch.tensor) -> tuple[torch.tensor, torch.tensor, torch.tensor]:
+                        slice_size: int, fmin: torch.tensor, zero: torch.tensor) -> tuple[torch.tensor, torch.tensor, torch.tensor]:
     """Partial attention where qkv are assumed to be causal between slices"""
     if bias is None:
         return (None, None, None)
@@ -174,7 +176,7 @@ def partial_attn_causal(query: torch.tensor, key: torch.tensor, value: torch.ten
         b = bias[q_min:q_max, 0:q_max]
 
         s_attn = torch.matmul(q, k.transpose(-1, -2)) + b.unsqueeze(0).unsqueeze(0)
-        inputM_hpu, inputL_hpu = create_softmax_fa2_input_tensors(s_attn, fmin)
+        inputM_hpu, inputL_hpu = create_softmax_fa2_input_tensors(s_attn, fmin, zero)
         s_attn, s_max, s_sum, _exp_max_fixup_hpu = torch.ops.hpu.softmax_fa2(s_attn,
                                                                              inputM=inputM_hpu,
                                                                              inputL=inputL_hpu)
@@ -192,7 +194,7 @@ def partial_attn_causal(query: torch.tensor, key: torch.tensor, value: torch.ten
     return combine(attn_slices), combine(max_slices), combine(sum_slices)
 
 
-def partial_attn_shared(query: torch.tensor, blocks: torch.tensor, bias: Optional[torch.tensor], fmin: torch.tensor,
+def partial_attn_shared(query: torch.tensor, blocks: torch.tensor, bias: Optional[torch.tensor], fmin: torch.tensor, zero: torch.tensor,
                         cache_utils: CacheUtils) -> tuple[torch.tensor, torch.tensor, torch.tensor]:
     """Partial attention where all shared blocks are compared with whole query"""
     if bias is None:
@@ -205,7 +207,8 @@ def partial_attn_shared(query: torch.tensor, blocks: torch.tensor, bias: Optiona
     attn = torch.matmul(query, key.transpose(-1, -2))
     attn = attn.flatten(0, 1)
     attn = attn + bias
-    inputM_hpu, inputL_hpu = create_softmax_fa2_input_tensors(attn, fmin)
+
+    inputM_hpu, inputL_hpu = create_softmax_fa2_input_tensors(attn, fmin, zero)
     attn, local_max, local_sum, _exp_max_fixup_hpu = torch.ops.hpu.softmax_fa2(attn,
                                                                                inputM=inputM_hpu,
                                                                                inputL=inputL_hpu)
@@ -258,6 +261,7 @@ class HPUUnifiedAttentionMetadata:
     unique_bias: Optional[torch.tensor]
     fmin: torch.tensor
     feps: torch.tensor
+    zero: torch.tensor
 
     def seq_len(self):
         # TODO: This needs to be changed in case of mixed batches
@@ -291,11 +295,13 @@ def unified_attn(query: torch.tensor, key: torch.tensor, value: torch.tensor, ke
                                  value=value,
                                  bias=metadata.causal_bias,
                                  slice_size=metadata.causal_width,
-                                 fmin=metadata.fmin)
+                                 fmin=metadata.fmin,
+                                 zero=metadata.zero)
     shared = partial_attn_shared(query=scaled_query,
                                  blocks=metadata.shared_blocks,
                                  bias=metadata.shared_bias,
                                  fmin=metadata.fmin,
+                                 zero=metadata.zero,
                                  cache_utils=cache_utils)
     unique = partial_attn_unique(query=scaled_query,
                                  blocks=metadata.unique_blocks,
diff --git a/vllm_gaudi/extension/unified_batch.py b/vllm_gaudi/extension/unified_batch.py
index ffd848a..5e63fb2 100644
--- a/vllm_gaudi/extension/unified_batch.py
+++ b/vllm_gaudi/extension/unified_batch.py
@@ -302,8 +302,9 @@ def create_unified_batch(req_ids: list[str], all_token_ids: torch.Tensor, num_co
     target_logits += get_dp_padding_fn(target_logits)
 
     default_causal_width = 512
-    fmin = torch.finfo(dtype).min
+    fmin = torch.tensor(torch.finfo(dtype).min, dtype=dtype, device="cpu")
     feps = torch.finfo(dtype).tiny
+    zero = torch.zeros((), dtype=dtype, device="cpu")
 
     # Convert numpy arrays to HPU tensors with proper dtypes
     return UnifiedBatch(req_ids_cpu=req_ids,
@@ -324,6 +325,7 @@ def create_unified_batch(req_ids: list[str], all_token_ids: torch.Tensor, num_co
                             unique_block_mapping=hpu_tensor(unique_block_mapping, (target_unique_blocks, ), -1,
                                                             slot_mapping_dtype),
                             unique_bias=hpu_tensor(unique_bias, (target_unique_blocks, block_size), -math.inf, dtype),
-                            fmin=to_hpu(fmin),
+                            fmin=fmin,
                             feps=to_hpu(feps),
+                            zero=zero,
                         ))
