Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
INFO 02-09 19:43:52 [plugins/__init__.py:43] Available plugins for group vllm.platform_plugins:
INFO 02-09 19:43:52 [plugins/__init__.py:45] - hpu -> vllm_gaudi:register
INFO 02-09 19:43:52 [plugins/__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 02-09 19:43:52 [platforms/__init__.py:36] Checking if TPU platform is available.
DEBUG 02-09 19:43:52 [platforms/__init__.py:55] TPU platform is not available because: No module named 'libtpu'
DEBUG 02-09 19:43:52 [platforms/__init__.py:61] Checking if CUDA platform is available.
DEBUG 02-09 19:43:52 [platforms/__init__.py:88] Exception happens when checking CUDA platform: NVML Shared Library Not Found
DEBUG 02-09 19:43:52 [platforms/__init__.py:105] CUDA platform is not available because: NVML Shared Library Not Found
DEBUG 02-09 19:43:52 [platforms/__init__.py:112] Checking if ROCm platform is available.
DEBUG 02-09 19:43:52 [platforms/__init__.py:126] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 02-09 19:43:52 [platforms/__init__.py:133] Checking if XPU platform is available.
DEBUG 02-09 19:43:52 [platforms/__init__.py:155] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 02-09 19:43:52 [platforms/__init__.py:162] Checking if CPU platform is available.
INFO 02-09 19:43:52 [platforms/__init__.py:219] Platform plugin hpu is activated
INFO 02-09 19:43:53 [triton_utils/importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.
INFO 02-09 19:43:53 [triton_utils/importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
DEBUG 02-09 19:43:54 [entrypoints/utils.py:187] Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'
DEBUG 02-09 19:43:54 [plugins/__init__.py:43] Available plugins for group vllm.general_plugins:
DEBUG 02-09 19:43:54 [plugins/__init__.py:45] - 01.hpu_custom_utils -> vllm_gaudi:register_utils
DEBUG 02-09 19:43:54 [plugins/__init__.py:45] - 02.hpu_custom_ops -> vllm_gaudi:register_ops
DEBUG 02-09 19:43:54 [plugins/__init__.py:45] - 03.hpu_custom_models -> vllm_gaudi:register_models
DEBUG 02-09 19:43:54 [plugins/__init__.py:45] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 02-09 19:43:54 [plugins/__init__.py:45] - lora_hf_hub_resolver -> vllm.plugins.lora_resolvers.hf_hub_resolver:register_hf_hub_resolver
DEBUG 02-09 19:43:54 [plugins/__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 02-09 19:43:54 [utils/flashinfer.py:55] FlashInfer unavailable since package was not found
WARNING 02-09 19:43:54 [platforms/interface.py:222] Failed to import from vllm._C: ModuleNotFoundError("No module named 'vllm._C'")
INFO 02-09 19:43:55 [distributed/.../v1/nixl_connector.py:101] Setting UCX_RCACHE_MAX_UNRELEASED to '1024' to avoid a rare memory leak in UCX when using NIXL.
WARNING 02-09 19:43:55 [distributed/.../v1/nixl_connector.py:116] NIXL is not available
WARNING 02-09 19:43:55 [distributed/.../v1/nixl_connector.py:128] NIXL agent config is not available
WARNING 02-09 19:43:55 [platform.py:163] Pin memory is not supported on HPU.
WARNING 02-09 19:43:55 [model_executor/models/registry.py:823] Model architecture Gemma3ForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_gaudi.models.gemma3_mm:HpuGemma3ForConditionalGeneration.
WARNING 02-09 19:43:55 [model_executor/models/registry.py:823] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_gaudi.models.qwen2_5_vl:HpuQwen2_5_VLForConditionalGeneration.
WARNING 02-09 19:43:55 [model_executor/models/registry.py:823] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_gaudi.models.qwen3_vl:HpuQwen3_VLForConditionalGeneration.
DEBUG 02-09 19:43:55 [compilation/decorators.py:202] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.qwen3_moe.Qwen3MoeModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
WARNING 02-09 19:43:55 [model_executor/models/registry.py:823] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_gaudi.models.qwen3_vl_moe:HpuQwen3_VLMoeForConditionalGeneration.
WARNING 02-09 19:43:55 [utils/argparse_utils.py:342] Found duplicate keys --trust-remote-code
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:43:55 [entrypoints/utils.py:346] 
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:43:55 [entrypoints/utils.py:346]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:43:55 [entrypoints/utils.py:346]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.0rc2.dev85+g17b17c068.d20260209
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:43:55 [entrypoints/utils.py:346]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:43:55 [entrypoints/utils.py:346]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:43:55 [entrypoints/utils.py:346] 
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:43:55 [entrypoints/utils.py:282] non-default args: {'host': 'localhost', 'port': 12346, 'model': '/root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/', 'trust_remote_code': True, 'max_model_len': 32768, 'gpu_memory_utilization': 0.75, 'limit_mm_per_prompt': {'image': {'count': 20, 'width': 864, 'height': 480}}, 'mm_processor_kwargs': {'size': {'shortest_edge': 65536, 'longest_edge': 1048576}}, 'max_num_batched_tokens': 32768}
[0;36m(APIServer pid=65602)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/config.json
[0;36m(APIServer pid=65602)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/config.json
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/config.json
[0;36m(APIServer pid=65602)[0;0m Model config Qwen3VLConfig {
[0;36m(APIServer pid=65602)[0;0m   "architectures": [
[0;36m(APIServer pid=65602)[0;0m     "Qwen3VLForConditionalGeneration"
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "image_token_id": 151655,
[0;36m(APIServer pid=65602)[0;0m   "model_type": "qwen3_vl",
[0;36m(APIServer pid=65602)[0;0m   "quantization_config": {
[0;36m(APIServer pid=65602)[0;0m     "activation_scheme": "dynamic",
[0;36m(APIServer pid=65602)[0;0m     "fmt": "e4m3",
[0;36m(APIServer pid=65602)[0;0m     "ignored_layers": [
[0;36m(APIServer pid=65602)[0;0m       "lm_head",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.merger.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.merger.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.merger.norm",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.patch_embed.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.pos_embed",
[0;36m(APIServer pid=65602)[0;0m       "visual.merger.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.merger.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.merger.norm",
[0;36m(APIServer pid=65602)[0;0m       "visual.patch_embed.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.pos_embed",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.0.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.0.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.0.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.0.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.0.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.0.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.0.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.0.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.1.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.1.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.1.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.1.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.1.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.1.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.1.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.1.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.2.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.2.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.2.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.2.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.2.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.2.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.2.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.2.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.3.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.3.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.3.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.3.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.3.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.3.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.3.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.3.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.4.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.4.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.4.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.4.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.4.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.4.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.4.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.4.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.5.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.5.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.5.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.5.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.5.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.5.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.5.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.5.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.6.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.6.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.6.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.6.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.6.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.6.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.6.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.6.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.7.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.7.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.7.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.7.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.7.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.7.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.7.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.7.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.8.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.8.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.8.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.8.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.8.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.8.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.8.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.8.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.9.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.9.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.9.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.9.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.9.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.9.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.9.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.9.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.10.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.10.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.10.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.10.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.10.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.10.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.10.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.10.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.11.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.11.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.11.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.11.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.11.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.11.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.11.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.11.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.12.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.12.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.12.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.12.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.12.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.12.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.12.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.12.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.13.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.13.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.13.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.13.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.13.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.13.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.13.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.13.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.14.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.14.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.14.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.14.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.14.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.14.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.14.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.14.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.15.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.15.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.15.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.15.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.15.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.15.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.15.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.15.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.16.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.16.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.16.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.16.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.16.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.16.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.16.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.16.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.17.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.17.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.17.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.17.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.17.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.17.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.17.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.17.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.18.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.18.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.18.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.18.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.18.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.18.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.18.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.18.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.19.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.19.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.19.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.19.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.19.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.19.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.19.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.19.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.20.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.20.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.20.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.20.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.20.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.20.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.20.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.20.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.21.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.21.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.21.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.21.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.21.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.21.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.21.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.21.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.22.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.22.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.22.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.22.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.22.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.22.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.22.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.22.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.23.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.23.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.23.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.23.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.23.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.23.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.23.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.23.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.24.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.24.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.24.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.24.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.24.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.24.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.24.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.24.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.25.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.25.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.25.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.25.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.25.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.25.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.25.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.25.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.26.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.26.attn.qkv",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.26.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.blocks.26.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.26.attn.proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.26.attn.qkv_proj",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.26.mlp.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.blocks.26.mlp.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.deepstack_merger_list.0.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.deepstack_merger_list.0.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.deepstack_merger_list.0.norm",
[0;36m(APIServer pid=65602)[0;0m       "visual.deepstack_merger_list.0.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.deepstack_merger_list.0.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.deepstack_merger_list.0.norm",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.deepstack_merger_list.1.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.deepstack_merger_list.1.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.deepstack_merger_list.1.norm",
[0;36m(APIServer pid=65602)[0;0m       "visual.deepstack_merger_list.1.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.deepstack_merger_list.1.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.deepstack_merger_list.1.norm",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.deepstack_merger_list.2.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.deepstack_merger_list.2.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "model.visual.deepstack_merger_list.2.norm",
[0;36m(APIServer pid=65602)[0;0m       "visual.deepstack_merger_list.2.linear_fc1",
[0;36m(APIServer pid=65602)[0;0m       "visual.deepstack_merger_list.2.linear_fc2",
[0;36m(APIServer pid=65602)[0;0m       "visual.deepstack_merger_list.2.norm"
[0;36m(APIServer pid=65602)[0;0m     ],
[0;36m(APIServer pid=65602)[0;0m     "quant_method": "fp8",
[0;36m(APIServer pid=65602)[0;0m     "weight_block_size": [
[0;36m(APIServer pid=65602)[0;0m       128,
[0;36m(APIServer pid=65602)[0;0m       128
[0;36m(APIServer pid=65602)[0;0m     ]
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "text_config": {
[0;36m(APIServer pid=65602)[0;0m     "attention_bias": false,
[0;36m(APIServer pid=65602)[0;0m     "attention_dropout": 0.0,
[0;36m(APIServer pid=65602)[0;0m     "bos_token_id": 151643,
[0;36m(APIServer pid=65602)[0;0m     "dtype": "bfloat16",
[0;36m(APIServer pid=65602)[0;0m     "eos_token_id": 151645,
[0;36m(APIServer pid=65602)[0;0m     "head_dim": 128,
[0;36m(APIServer pid=65602)[0;0m     "hidden_act": "silu",
[0;36m(APIServer pid=65602)[0;0m     "hidden_size": 5120,
[0;36m(APIServer pid=65602)[0;0m     "initializer_range": 0.02,
[0;36m(APIServer pid=65602)[0;0m     "intermediate_size": 25600,
[0;36m(APIServer pid=65602)[0;0m     "max_position_embeddings": 262144,
[0;36m(APIServer pid=65602)[0;0m     "model_type": "qwen3_vl_text",
[0;36m(APIServer pid=65602)[0;0m     "num_attention_heads": 64,
[0;36m(APIServer pid=65602)[0;0m     "num_hidden_layers": 64,
[0;36m(APIServer pid=65602)[0;0m     "num_key_value_heads": 8,
[0;36m(APIServer pid=65602)[0;0m     "rms_norm_eps": 1e-06,
[0;36m(APIServer pid=65602)[0;0m     "rope_scaling": {
[0;36m(APIServer pid=65602)[0;0m       "mrope_interleaved": true,
[0;36m(APIServer pid=65602)[0;0m       "mrope_section": [
[0;36m(APIServer pid=65602)[0;0m         24,
[0;36m(APIServer pid=65602)[0;0m         20,
[0;36m(APIServer pid=65602)[0;0m         20
[0;36m(APIServer pid=65602)[0;0m       ],
[0;36m(APIServer pid=65602)[0;0m       "rope_type": "default"
[0;36m(APIServer pid=65602)[0;0m     },
[0;36m(APIServer pid=65602)[0;0m     "rope_theta": 5000000,
[0;36m(APIServer pid=65602)[0;0m     "use_cache": true,
[0;36m(APIServer pid=65602)[0;0m     "vocab_size": 151936
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "tie_word_embeddings": false,
[0;36m(APIServer pid=65602)[0;0m   "transformers_version": "4.57.6",
[0;36m(APIServer pid=65602)[0;0m   "video_token_id": 151656,
[0;36m(APIServer pid=65602)[0;0m   "vision_config": {
[0;36m(APIServer pid=65602)[0;0m     "deepstack_visual_indexes": [
[0;36m(APIServer pid=65602)[0;0m       8,
[0;36m(APIServer pid=65602)[0;0m       16,
[0;36m(APIServer pid=65602)[0;0m       24
[0;36m(APIServer pid=65602)[0;0m     ],
[0;36m(APIServer pid=65602)[0;0m     "depth": 27,
[0;36m(APIServer pid=65602)[0;0m     "hidden_act": "gelu_pytorch_tanh",
[0;36m(APIServer pid=65602)[0;0m     "hidden_size": 1152,
[0;36m(APIServer pid=65602)[0;0m     "in_channels": 3,
[0;36m(APIServer pid=65602)[0;0m     "initializer_range": 0.02,
[0;36m(APIServer pid=65602)[0;0m     "intermediate_size": 4304,
[0;36m(APIServer pid=65602)[0;0m     "model_type": "qwen3_vl",
[0;36m(APIServer pid=65602)[0;0m     "num_heads": 16,
[0;36m(APIServer pid=65602)[0;0m     "num_position_embeddings": 2304,
[0;36m(APIServer pid=65602)[0;0m     "out_hidden_size": 5120,
[0;36m(APIServer pid=65602)[0;0m     "patch_size": 16,
[0;36m(APIServer pid=65602)[0;0m     "spatial_merge_size": 2,
[0;36m(APIServer pid=65602)[0;0m     "temporal_patch_size": 2
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "vision_end_token_id": 151653,
[0;36m(APIServer pid=65602)[0;0m   "vision_start_token_id": 151652
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:43:55 [model_executor/models/registry.py:735] Loaded model info for class vllm_gaudi.models.qwen3_vl.HpuQwen3_VLForConditionalGeneration from cache
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:43:55 [logging_utils/log_time.py:29] Registry inspect model class: Elapsed time 0.0005849 secs
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:43:55 [config/model.py:541] Resolved architecture: Qwen3VLForConditionalGeneration
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:43:55 [config/model.py:1558] Using max model len 32768
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:43:55 [_ipex_ops.py:15] Import error msg: No module named 'intel_extension_for_pytorch'
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:43:55 [config/model.py:1622] Generative models support chunked prefill.
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:43:55 [config/model.py:1680] Generative models support prefix caching.
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:43:55 [engine/arg_utils.py:1900] Enabling chunked prefill by default
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:43:55 [engine/arg_utils.py:1930] Enabling prefix caching by default
[0;36m(APIServer pid=65602)[0;0m WARNING 02-09 19:43:55 [platform.py:95] This is a workaround! Please check the NOTE in the get_device_total_memory definition.
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:43:55 [engine/arg_utils.py:2018] Defaulting max_num_seqs to 256 for OPENAI_API_SERVER usage context.
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:43:55 [config/parallel.py:701] Disabled the custom all-reduce kernel because it is not supported on current platform.
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:43:55 [config/scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=32768.
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:43:55 [config/parallel.py:701] Disabled the custom all-reduce kernel because it is not supported on current platform.
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:43:55 [config/vllm.py:633] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=65602)[0;0m WARNING 02-09 19:43:55 [platform.py:143] Using Contiguous PA, disabling prefix caching
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:43:55 [platform.py:147] [HPU] Forcing CompilationMode.NONE compilation mode
[0;36m(APIServer pid=65602)[0;0m =========compilation_config.custom_ops=['all']===========
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:43:55 [plugins/__init__.py:35] No plugins for group vllm.stat_logger_plugins found.
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/generation_config.json
[0;36m(APIServer pid=65602)[0;0m Generate config GenerationConfig {
[0;36m(APIServer pid=65602)[0;0m   "bos_token_id": 151643,
[0;36m(APIServer pid=65602)[0;0m   "do_sample": true,
[0;36m(APIServer pid=65602)[0;0m   "eos_token_id": [
[0;36m(APIServer pid=65602)[0;0m     151645,
[0;36m(APIServer pid=65602)[0;0m     151643
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "pad_token_id": 151643,
[0;36m(APIServer pid=65602)[0;0m   "temperature": 0.7,
[0;36m(APIServer pid=65602)[0;0m   "top_k": 20,
[0;36m(APIServer pid=65602)[0;0m   "top_p": 0.8
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:43:55 [tokenizers/registry.py:64] Loading CachedHfTokenizer for tokenizer_mode='hf'
[0;36m(APIServer pid=65602)[0;0m loading file vocab.json
[0;36m(APIServer pid=65602)[0;0m loading file merges.txt
[0;36m(APIServer pid=65602)[0;0m loading file tokenizer.json
[0;36m(APIServer pid=65602)[0;0m loading file added_tokens.json
[0;36m(APIServer pid=65602)[0;0m loading file special_tokens_map.json
[0;36m(APIServer pid=65602)[0;0m loading file tokenizer_config.json
[0;36m(APIServer pid=65602)[0;0m loading file chat_template.jinja
[0;36m(APIServer pid=65602)[0;0m Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:43:55 [renderers/registry.py:51] Loading HfRenderer for renderer_mode='hf'
[0;36m(APIServer pid=65602)[0;0m loading file vocab.json
[0;36m(APIServer pid=65602)[0;0m loading file merges.txt
[0;36m(APIServer pid=65602)[0;0m loading file tokenizer.json
[0;36m(APIServer pid=65602)[0;0m loading file added_tokens.json
[0;36m(APIServer pid=65602)[0;0m loading file special_tokens_map.json
[0;36m(APIServer pid=65602)[0;0m loading file tokenizer_config.json
[0;36m(APIServer pid=65602)[0;0m loading file chat_template.jinja
[0;36m(APIServer pid=65602)[0;0m Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:43:56 [plugins/io_processors/__init__.py:33] No IOProcessor plugins requested by the model
Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)
INFO 02-09 19:43:59 [plugins/__init__.py:43] Available plugins for group vllm.platform_plugins:
INFO 02-09 19:43:59 [plugins/__init__.py:45] - hpu -> vllm_gaudi:register
INFO 02-09 19:43:59 [plugins/__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
DEBUG 02-09 19:43:59 [platforms/__init__.py:36] Checking if TPU platform is available.
DEBUG 02-09 19:43:59 [platforms/__init__.py:55] TPU platform is not available because: No module named 'libtpu'
DEBUG 02-09 19:43:59 [platforms/__init__.py:61] Checking if CUDA platform is available.
DEBUG 02-09 19:43:59 [platforms/__init__.py:88] Exception happens when checking CUDA platform: NVML Shared Library Not Found
DEBUG 02-09 19:43:59 [platforms/__init__.py:105] CUDA platform is not available because: NVML Shared Library Not Found
DEBUG 02-09 19:43:59 [platforms/__init__.py:112] Checking if ROCm platform is available.
DEBUG 02-09 19:43:59 [platforms/__init__.py:126] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 02-09 19:43:59 [platforms/__init__.py:133] Checking if XPU platform is available.
DEBUG 02-09 19:43:59 [platforms/__init__.py:155] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 02-09 19:43:59 [platforms/__init__.py:162] Checking if CPU platform is available.
INFO 02-09 19:43:59 [platforms/__init__.py:219] Platform plugin hpu is activated
INFO 02-09 19:44:00 [triton_utils/importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.
INFO 02-09 19:44:00 [triton_utils/importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
DEBUG 02-09 19:44:02 [utils/flashinfer.py:55] FlashInfer unavailable since package was not found
WARNING 02-09 19:44:02 [platforms/interface.py:222] Failed to import from vllm._C: ModuleNotFoundError("No module named 'vllm._C'")
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:02 [v1/engine/core.py:861] Waiting for init message from front-end.
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:44:02 [v1/engine/utils.py:1093] HELLO from local core engine process 0.
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:02 [v1/engine/core.py:872] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/c5d6190b-a54b-467f-b24e-93b59f43dc18'], outputs=['ipc:///tmp/ca13534c-f32e-494c-babd-ac0608005aff'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={})
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:02 [v1/engine/core.py:676] Has DP Coordinator: False, stats publish address: None
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:02 [plugins/__init__.py:43] Available plugins for group vllm.general_plugins:
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:02 [plugins/__init__.py:45] - 01.hpu_custom_utils -> vllm_gaudi:register_utils
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:02 [plugins/__init__.py:45] - 02.hpu_custom_ops -> vllm_gaudi:register_ops
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:02 [plugins/__init__.py:45] - 03.hpu_custom_models -> vllm_gaudi:register_models
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:02 [plugins/__init__.py:45] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:02 [plugins/__init__.py:45] - lora_hf_hub_resolver -> vllm.plugins.lora_resolvers.hf_hub_resolver:register_hf_hub_resolver
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:02 [plugins/__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[0;36m(EngineCore_DP0 pid=65986)[0;0m WARNING 02-09 19:44:02 [distributed/.../v1/nixl_connector.py:116] NIXL is not available
[0;36m(EngineCore_DP0 pid=65986)[0;0m WARNING 02-09 19:44:02 [distributed/.../v1/nixl_connector.py:128] NIXL agent config is not available
[0;36m(EngineCore_DP0 pid=65986)[0;0m WARNING 02-09 19:44:02 [platform.py:163] Pin memory is not supported on HPU.
[0;36m(EngineCore_DP0 pid=65986)[0;0m WARNING 02-09 19:44:02 [model_executor/models/registry.py:823] Model architecture Gemma3ForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_gaudi.models.gemma3_mm:HpuGemma3ForConditionalGeneration.
[0;36m(EngineCore_DP0 pid=65986)[0;0m WARNING 02-09 19:44:02 [model_executor/models/registry.py:823] Model architecture Qwen2_5_VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_gaudi.models.qwen2_5_vl:HpuQwen2_5_VLForConditionalGeneration.
[0;36m(EngineCore_DP0 pid=65986)[0;0m WARNING 02-09 19:44:02 [model_executor/models/registry.py:823] Model architecture Qwen3VLForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_gaudi.models.qwen3_vl:HpuQwen3_VLForConditionalGeneration.
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:02 [compilation/decorators.py:202] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.qwen3_moe.Qwen3MoeModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[0;36m(EngineCore_DP0 pid=65986)[0;0m WARNING 02-09 19:44:02 [model_executor/models/registry.py:823] Model architecture Qwen3VLMoeForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_gaudi.models.qwen3_vl_moe:HpuQwen3_VLMoeForConditionalGeneration.
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:02 [v1/engine/core.py:96] Initializing a V1 LLM engine (v0.15.0rc2.dev85+g17b17c068.d20260209) with config: model='/root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/', speculative_config=None, tokenizer='/root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=fp8, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=hpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'hpu_backend', 'custom_ops': ['all', '+quant_fp8'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': [32768], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:02 [compilation/decorators.py:202] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:02 [compilation/decorators.py:202] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:02 [tokenizers/registry.py:64] Loading CachedHfTokenizer for tokenizer_mode='hf'
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file vocab.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file merges.txt
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file tokenizer.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file added_tokens.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file special_tokens_map.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file tokenizer_config.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file chat_template.jinja
[0;36m(EngineCore_DP0 pid=65986)[0;0m Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:03 [hpu_worker.py:125] HABANA_VISIBLE_MODULES is not set, using all available modules: 1
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:03 [distributed/parallel_state.py:1192] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.26.46.50:59559 backend=hccl
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:03 [distributed/parallel_state.py:1234] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.26.46.50:59559 backend=hccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:03 [distributed/parallel_state.py:1278] Detected 1 nodes in the distributed environment
============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= 
 PT_HPU_LAZY_MODE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
 PT_HPU_ENABLE_LAZY_COLLECTIVES = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 224
CPU RAM       : 1007 GB
------------------------------------------------------------------------------
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:03 [distributed/parallel_state.py:1445] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:28] Environment:
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     hw: gaudi3
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     build: 1.23.0.695
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     engine_version: v1
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     bridge_mode: eager
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     model_type: qwen3_vl
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     prefix_caching: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     vllm_gaudi_commit: libinta/remove_gather_scatter+c628c92
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:28] Features:
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     fp32_alibi_biases: True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     fp32_softmax: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     fused_block_softmax_adjustment: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     fused_block_softmax: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     prompt_attn_impl: fsdpa_impl
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     skip_warmup: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     merged_prefill: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     use_contiguous_pa: True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     use_bucketing: True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     bucketing_strategy: linear_bucketing
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     defrag: True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     regional_compilation: True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     dynamic_shapes_compilation: True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     fullgraph_compilation: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     unified_attn: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     unified_attn_dense_shared_bias: True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     unified_attn_chunked_shared_attn: True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     unified_attn_online_merge: True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     unified_attn_shared_attn_chunk_size: 64
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     unified_attn_split_graphs: True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     unified_attn_softmax_fa2: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     scale_adjustment: True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     flatten_input: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     unified_attn_shared_cache_ratio: 1
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     high_level_profiler_enabled: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     track_graph_compilation: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     use_output_tensor_in_matmulqk: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     per_token_kv_scaling_support: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     moe_chunk: 
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     moe_token_boundary: 
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     use_dispatch_fn: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     use_hpu_aligned_scale: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:28] User flags:
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_EXPONENTIAL_BUCKETING: False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_PROMPT_BS_BUCKET_MIN: 1
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_PROMPT_BS_BUCKET_STEP: 1
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_PROMPT_BS_BUCKET_MAX: 1
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_PROMPT_SEQ_BUCKET_MIN: 5120
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_PROMPT_SEQ_BUCKET_STEP: 1024
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_PROMPT_SEQ_BUCKET_MAX: 20480
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_PROMPT_CTX_BUCKET_MIN: 0
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_PROMPT_CTX_BUCKET_STEP: 12
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_PROMPT_CTX_BUCKET_MAX: 24
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_DECODE_BS_BUCKET_MIN: 1
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_DECODE_BS_BUCKET_STEP: 4
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_DECODE_BS_BUCKET_MAX: 64
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_DECODE_BLOCK_BUCKET_MIN: 256
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_DECODE_BLOCK_BUCKET_STEP: 128
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     VLLM_DECODE_BLOCK_BUCKET_MAX: 1152
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     EXPERIMENTAL_WEIGHT_SHARING: 0
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     PT_HPU_WEIGHT_SHARING: 0
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [runtime.py:32]     RUNTIME_SCALE_PATCHING: 1
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831] model config: ModelConfig(model='/root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/', model_weights='', runner='auto', convert='auto', tokenizer='/root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/', tokenizer_mode='auto', trust_remote_code=True, dtype=torch.bfloat16, seed=0, hf_config=Qwen3VLConfig {
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "architectures": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "Qwen3VLForConditionalGeneration"
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "image_token_id": 151655,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "model_type": "qwen3_vl",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "quantization_config": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "activation_scheme": "dynamic",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "fmt": "e4m3",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "ignored_layers": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "lm_head",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.merger.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.merger.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.merger.norm",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.patch_embed.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.pos_embed",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.merger.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.merger.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.merger.norm",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.patch_embed.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.pos_embed",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.0.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.0.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.0.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.0.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.0.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.0.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.0.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.0.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.1.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.1.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.1.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.1.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.1.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.1.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.1.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.1.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.2.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.2.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.2.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.2.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.2.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.2.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.2.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.2.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.3.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.3.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.3.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.3.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.3.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.3.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.3.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.3.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.4.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.4.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.4.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.4.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.4.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.4.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.4.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.4.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.5.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.5.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.5.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.5.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.5.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.5.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.5.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.5.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.6.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.6.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.6.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.6.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.6.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.6.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.6.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.6.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.7.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.7.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.7.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.7.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.7.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.7.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.7.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.7.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.8.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.8.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.8.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.8.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.8.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.8.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.8.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.8.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.9.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.9.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.9.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.9.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.9.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.9.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.9.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.9.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.10.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.10.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.10.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.10.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.10.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.10.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.10.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.10.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.11.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.11.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.11.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.11.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.11.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.11.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.11.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.11.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.12.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.12.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.12.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.12.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.12.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.12.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.12.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.12.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.13.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.13.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.13.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.13.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.13.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.13.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.13.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.13.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.14.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.14.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.14.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.14.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.14.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.14.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.14.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.14.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.15.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.15.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.15.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.15.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.15.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.15.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.15.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.15.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.16.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.16.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.16.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.16.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.16.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.16.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.16.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.16.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.17.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.17.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.17.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.17.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.17.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.17.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.17.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.17.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.18.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.18.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.18.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.18.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.18.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.18.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.18.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.18.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.19.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.19.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.19.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.19.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.19.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.19.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.19.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.19.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.20.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.20.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.20.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.20.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.20.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.20.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.20.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.20.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.21.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.21.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.21.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.21.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.21.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.21.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.21.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.21.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.22.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.22.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.22.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.22.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.22.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.22.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.22.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.22.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.23.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.23.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.23.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.23.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.23.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.23.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.23.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.23.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.24.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.24.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.24.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.24.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.24.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.24.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.24.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.24.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.25.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.25.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.25.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.25.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.25.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.25.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.25.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.25.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.26.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.26.attn.qkv",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.26.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.blocks.26.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.26.attn.proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.26.attn.qkv_proj",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.26.mlp.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.blocks.26.mlp.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.deepstack_merger_list.0.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.deepstack_merger_list.0.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.deepstack_merger_list.0.norm",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.deepstack_merger_list.0.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.deepstack_merger_list.0.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.deepstack_merger_list.0.norm",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.deepstack_merger_list.1.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.deepstack_merger_list.1.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.deepstack_merger_list.1.norm",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.deepstack_merger_list.1.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.deepstack_merger_list.1.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.deepstack_merger_list.1.norm",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.deepstack_merger_list.2.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.deepstack_merger_list.2.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "model.visual.deepstack_merger_list.2.norm",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.deepstack_merger_list.2.linear_fc1",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.deepstack_merger_list.2.linear_fc2",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "visual.deepstack_merger_list.2.norm"
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "quant_method": "fp8",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "weight_block_size": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       128,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       128
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     ]
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   },
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "text_config": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "attention_bias": false,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "attention_dropout": 0.0,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "bos_token_id": 151643,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "dtype": "bfloat16",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "eos_token_id": 151645,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "head_dim": 128,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "hidden_act": "silu",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "hidden_size": 5120,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "initializer_range": 0.02,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "intermediate_size": 25600,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "max_position_embeddings": 262144,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "model_type": "qwen3_vl_text",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "num_attention_heads": 64,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "num_hidden_layers": 64,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "num_key_value_heads": 8,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "rms_norm_eps": 1e-06,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "rope_parameters": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "mrope_interleaved": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "mrope_section": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]         24,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]         20,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]         20
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "rope_theta": 5000000,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "rope_type": "default"
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     },
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "rope_scaling": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "mrope_interleaved": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "mrope_section": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]         24,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]         20,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]         20
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "rope_theta": 5000000,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       "rope_type": "default"
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     },
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "rope_theta": 5000000,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "use_cache": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "vocab_size": 151936
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   },
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "tie_word_embeddings": false,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "transformers_version": "4.57.6",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "video_token_id": 151656,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "vision_config": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "deepstack_visual_indexes": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       8,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       16,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       24
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "depth": 27,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "hidden_act": "gelu_pytorch_tanh",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "hidden_size": 1152,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "in_channels": 3,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "initializer_range": 0.02,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "intermediate_size": 4304,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "model_type": "qwen3_vl",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "num_heads": 16,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "num_position_embeddings": 2304,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "out_hidden_size": 5120,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "patch_size": 16,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "spatial_merge_size": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "temporal_patch_size": 2
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   },
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "vision_end_token_id": 151653,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "vision_start_token_id": 151652
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831] }
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831] , hf_text_config=Qwen3VLTextConfig {
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "attention_bias": false,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "attention_dropout": 0.0,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "bos_token_id": 151643,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "dtype": "bfloat16",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "eos_token_id": 151645,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "head_dim": 128,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "hidden_act": "silu",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "hidden_size": 5120,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "initializer_range": 0.02,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "intermediate_size": 25600,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "max_position_embeddings": 262144,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "model_type": "qwen3_vl_text",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "num_attention_heads": 64,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "num_hidden_layers": 64,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "num_key_value_heads": 8,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "rms_norm_eps": 1e-06,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "rope_parameters": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "mrope_interleaved": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "mrope_section": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       24,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       20,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       20
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "rope_theta": 5000000,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "rope_type": "default"
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   },
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "rope_scaling": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "mrope_interleaved": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "mrope_section": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       24,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       20,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]       20
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "rope_theta": 5000000,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]     "rope_type": "default"
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   },
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "rope_theta": 5000000,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "tie_word_embeddings": false,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "transformers_version": "4.57.6",
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "use_cache": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831]   "vocab_size": 151936
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831] }
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [hpu_model_runner.py:831] , hf_config_path=None, allowed_local_media_path='', allowed_media_domains=None, revision=None, code_revision=None, tokenizer_revision=None, max_model_len=32768, spec_target_max_model_len=None, quantization='fp8', allow_deprecated_quantization=False, enforce_eager=False, enable_return_routed_experts=False, max_logprobs=20, logprobs_mode='raw_logprobs', disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='/root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/', config_format='auto', hf_token=None, hf_overrides={}, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto', override_attention_dtype=None, logits_processors=None, io_processor_plugin=None, pooler_config=None, multimodal_config=MultiModalConfig(limit_per_prompt={'image': ImageDummyOptions(count=20, width=864, height=480)}, enable_mm_embeds=False, media_io_kwargs={}, mm_processor_kwargs={'size': {'shortest_edge': 65536, 'longest_edge': 1048576}}, mm_processor_cache_gb=4.0, mm_processor_cache_type='lru', mm_shm_cache_max_object_size_mb=128, mm_encoder_only=False, mm_encoder_tp_mode='weights', mm_encoder_attn_backend=None, interleave_mm_strings=False, skip_mm_profiling=False, video_pruning_rate=None))
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [platform.py:65] Using HPUAttentionV1 backend.
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [v1/sample/logits_processor/__init__.py:63] No logitsprocs plugins installed (group vllm.logits_processors).
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [hpu_model_runner.py:950] Bucketing is ON.
[0;36m(EngineCore_DP0 pid=65986)[0;0m WARNING 02-09 19:44:04 [hpu_model_runner.py:729] Setting OMP_NUM_THREADS to 224 and torch.set_num_threads to 224 for 224 available CPU cores and world size 1
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [hpu_model_runner.py:4112] Starting to load model /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/...
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: Conv3dLayer using <class 'vllm_gaudi.ops.hpu_conv.HPUConv3dLayer'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RotaryEmbedding using <class 'vllm_gaudi.ops.hpu_rotary_embedding.HPURotaryEmbedding'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [platforms/interface.py:267] Using default backend AttentionBackendEnum.TORCH_SDPA for vit attention
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [model_executor/.../attention/mm_encoder_attention.py:77] Using AttentionBackendEnum.TORCH_SDPA for MMEncoderAttention.
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [utils/deep_gemm.py:86] DeepGEMM E8M0 disabled: DeepGEMM not supported on this system.
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MRotaryEmbedding using <class 'vllm_gaudi.ops.hpu_rotary_embedding.HPUMRotaryEmbedding'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:04 [platform.py:65] Using HPUAttentionV1 backend.
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: RMSNorm using <class 'vllm_gaudi.ops.hpu_layernorm.HPURMSNorm'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: Conv3dLayer using <class 'vllm_gaudi.ops.hpu_conv.HPUConv3dLayer'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/custom_op.py:113] Instantiating custom op: MMEncoderAttention using <class 'vllm_gaudi.ops.hpu_mm_encoder_attention.HpuMMEncoderAttention'>
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [config/compilation.py:1039] enabled custom ops: Counter({'RMSNorm': 257, 'quant_fp8': 256, 'apply_rotary_emb': 137, 'MMEncoderAttention': 108, 'silu_and_mul': 64, 'Conv3dLayer': 2, 'RotaryEmbedding': 1, 'vocab_parallel_embedding': 1, 'MRotaryEmbedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [config/compilation.py:1040] disabled custom ops: Counter()
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:04 [model_executor/model_loader/base_loader.py:56] Loading weights on hpu ...
[0;36m(EngineCore_DP0 pid=65986)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=65986)[0;0m Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:04,  1.38it/s]
[0;36m(EngineCore_DP0 pid=65986)[0;0m Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:01<00:04,  1.04it/s]
[0;36m(EngineCore_DP0 pid=65986)[0;0m Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:02<00:04,  1.03s/it]
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:44:08 [model_executor/models/utils.py:222] Loaded weight lm_head.weight with shape torch.Size([151936, 5120])
[0;36m(EngineCore_DP0 pid=65986)[0;0m Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:04<00:03,  1.13s/it]
[0;36m(EngineCore_DP0 pid=65986)[0;0m Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:05<00:02,  1.12s/it]
[0;36m(EngineCore_DP0 pid=65986)[0;0m Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:06<00:01,  1.12s/it]
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:44:12 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
[0;36m(EngineCore_DP0 pid=65986)[0;0m Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:07<00:00,  1.11s/it]
[0;36m(EngineCore_DP0 pid=65986)[0;0m Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:07<00:00,  1.08s/it]
[0;36m(EngineCore_DP0 pid=65986)[0;0m 
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:12 [model_executor/model_loader/default_loader.py:291] Loading weights took 7.61 seconds
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:12 [hpu_model_runner.py:4118] Loading model weights took 34.7600 GB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:13 [hpu_model_runner.py:4160] Wrapping in HPUGraph took 0.0000 GB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:13 [hpu_model_runner.py:4188] Compilation took 0.0000 GB
[0;36m(EngineCore_DP0 pid=65986)[0;0m WARNING 02-09 19:44:13 [common.py:247] Prompt bucket for (1, 32768, 0) was not prepared. Adding new bucket: (1, 32768, 0)
[0;36m(EngineCore_DP0 pid=65986)[0;0m /usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:1692: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[0;36m(EngineCore_DP0 pid=65986)[0;0m   torch._dynamo.utils.warn_once(msg)
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:44:22 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:44:32 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:44:42 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:44:52 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [hpu_worker.py:308] Model profiling run took 9.042 GiB of device memory (43.84 GiB/126.5 GiB used) and 864 MiB of host memory (75.86 GiB/1007 GiB used)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [hpu_worker.py:332] Free device memory: 82.7 GiB, 62.03 GiB usable (gpu_memory_utilization=0.75), 24.81 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.4), 32 MiB reserved for KV cache dummy block 37.18 GiB reserved for usable KV cache
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [v1/core/kv_cache_utils.py:1307] GPU KV cache size: 152,192 tokens
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [v1/core/kv_cache_utils.py:1312] Maximum concurrency for 32,768 tokens per request: 4.64x
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [hpu_worker.py:367] Usable num_blocks: 1189, actual allocated num_blocks: 152320 (_PAD_BLOCK_ID=1189, _PAD_SLOT_ID=152192)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [hpu_worker.py:370] Initializing cache engine took 37.19 GiB of device memory (81 GiB/126.5 GiB used) and 4.102 MiB of host memory (75.86 GiB/1007 GiB used)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_PROMPT_BS_BUCKET_MIN=1 (default:1)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_PROMPT_BS_BUCKET_STEP=1 (default:1)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_PROMPT_BS_BUCKET_MAX=1 (default:1)
[0;36m(EngineCore_DP0 pid=65986)[0;0m WARNING 02-09 19:44:52 [linear.py:113] VLLM_PROMPT_QUERY_BUCKET_MIN not set, using VLLM_PROMPT_SEQ_BUCKET_MIN value (5120) instead. This fallback behavior is deprecated and will be removed in v0.12.0.
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_PROMPT_QUERY_BUCKET_MIN=5120 (default:128)
[0;36m(EngineCore_DP0 pid=65986)[0;0m WARNING 02-09 19:44:52 [linear.py:113] VLLM_PROMPT_QUERY_BUCKET_STEP not set, using VLLM_PROMPT_SEQ_BUCKET_STEP value (1024) instead. This fallback behavior is deprecated and will be removed in v0.12.0.
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_PROMPT_QUERY_BUCKET_STEP=1024 (default:128)
[0;36m(EngineCore_DP0 pid=65986)[0;0m WARNING 02-09 19:44:52 [linear.py:113] VLLM_PROMPT_QUERY_BUCKET_MAX not set, using VLLM_PROMPT_SEQ_BUCKET_MAX value (20480) instead. This fallback behavior is deprecated and will be removed in v0.12.0.
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_PROMPT_QUERY_BUCKET_MAX=20480 (default:32768)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_PROMPT_CTX_BUCKET_MIN=0 (default:0)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_PROMPT_CTX_BUCKET_STEP=12 (default:1)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_PROMPT_CTX_BUCKET_MAX=24 (default:216)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:52] Prompt bucket config (min, step, max_warmup) bs:[1, 1, 1], query:[5120, 1024, 20480], blocks:[0, 12, 24]
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [common.py:222] Generated 48 prompt buckets [bs, query, num_blocks]: [(1, 5120, 0), (1, 5120, 12), (1, 5120, 216), (1, 6144, 0), (1, 6144, 12), (1, 6144, 208), (1, 7168, 0), (1, 7168, 12), (1, 7168, 200), (1, 8192, 0), (1, 8192, 12), (1, 8192, 192), (1, 9216, 0), (1, 9216, 12), (1, 9216, 184), (1, 10240, 0), (1, 10240, 12), (1, 10240, 176), (1, 11264, 0), (1, 11264, 12), (1, 11264, 168), (1, 12288, 0), (1, 12288, 12), (1, 12288, 160), (1, 13312, 0), (1, 13312, 12), (1, 13312, 152), (1, 14336, 0), (1, 14336, 12), (1, 14336, 144), (1, 15360, 0), (1, 15360, 12), (1, 15360, 136), (1, 16384, 0), (1, 16384, 12), (1, 16384, 128), (1, 17408, 0), (1, 17408, 12), (1, 17408, 120), (1, 18432, 0), (1, 18432, 12), (1, 18432, 112), (1, 19456, 0), (1, 19456, 12), (1, 19456, 104), (1, 20480, 0), (1, 20480, 12), (1, 20480, 96)]
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_DECODE_BS_BUCKET_MIN=1 (default:1)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_DECODE_BS_BUCKET_STEP=4 (default:32)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_DECODE_BS_BUCKET_MAX=64 (default:256)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_DECODE_BLOCK_BUCKET_MIN=256 (default:1)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_DECODE_BLOCK_BUCKET_STEP=128 (default:128)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:116] VLLM_DECODE_BLOCK_BUCKET_MAX=1152 (default:1189)
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [linear.py:81] Decode bucket config (min, step, max_warmup) bs:[1, 4, 64], blocks:[256, 128, 1152]
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [common.py:222] Generated 162 decode buckets [bs, query, num_blocks]: [(1, 1, 256), (1, 1, 384), (1, 1, 512), (1, 1, 640), (1, 1, 768), (1, 1, 896), (1, 1, 1024), (1, 1, 1152), (1, 1, 1189), (2, 1, 256), (2, 1, 384), (2, 1, 512), (2, 1, 640), (2, 1, 768), (2, 1, 896), (2, 1, 1024), (2, 1, 1152), (2, 1, 1189), (4, 1, 256), (4, 1, 384), (4, 1, 512), (4, 1, 640), (4, 1, 768), (4, 1, 896), (4, 1, 1024), (4, 1, 1152), (4, 1, 1189), (8, 1, 256), (8, 1, 384), (8, 1, 512), (8, 1, 640), (8, 1, 768), (8, 1, 896), (8, 1, 1024), (8, 1, 1152), (8, 1, 1189), (12, 1, 256), (12, 1, 384), (12, 1, 512), (12, 1, 640), (12, 1, 768), (12, 1, 896), (12, 1, 1024), (12, 1, 1152), (12, 1, 1189), (16, 1, 256), (16, 1, 384), (16, 1, 512), (16, 1, 640), (16, 1, 768), (16, 1, 896), (16, 1, 1024), (16, 1, 1152), (16, 1, 1189), (20, 1, 256), (20, 1, 384), (20, 1, 512), (20, 1, 640), (20, 1, 768), (20, 1, 896), (20, 1, 1024), (20, 1, 1152), (20, 1, 1189), (24, 1, 256), (24, 1, 384), (24, 1, 512), (24, 1, 640), (24, 1, 768), (24, 1, 896), (24, 1, 1024), (24, 1, 1152), (24, 1, 1189), (28, 1, 256), (28, 1, 384), (28, 1, 512), (28, 1, 640), (28, 1, 768), (28, 1, 896), (28, 1, 1024), (28, 1, 1152), (28, 1, 1189), (32, 1, 256), (32, 1, 384), (32, 1, 512), (32, 1, 640), (32, 1, 768), (32, 1, 896), (32, 1, 1024), (32, 1, 1152), (32, 1, 1189), (36, 1, 256), (36, 1, 384), (36, 1, 512), (36, 1, 640), (36, 1, 768), (36, 1, 896), (36, 1, 1024), (36, 1, 1152), (36, 1, 1189), (40, 1, 256), (40, 1, 384), (40, 1, 512), (40, 1, 640), (40, 1, 768), (40, 1, 896), (40, 1, 1024), (40, 1, 1152), (40, 1, 1189), (44, 1, 256), (44, 1, 384), (44, 1, 512), (44, 1, 640), (44, 1, 768), (44, 1, 896), (44, 1, 1024), (44, 1, 1152), (44, 1, 1189), (48, 1, 256), (48, 1, 384), (48, 1, 512), (48, 1, 640), (48, 1, 768), (48, 1, 896), (48, 1, 1024), (48, 1, 1152), (48, 1, 1189), (52, 1, 256), (52, 1, 384), (52, 1, 512), (52, 1, 640), (52, 1, 768), (52, 1, 896), (52, 1, 1024), (52, 1, 1152), (52, 1, 1189), (56, 1, 256), (56, 1, 384), (56, 1, 512), (56, 1, 640), (56, 1, 768), (56, 1, 896), (56, 1, 1024), (56, 1, 1152), (56, 1, 1189), (60, 1, 256), (60, 1, 384), (60, 1, 512), (60, 1, 640), (60, 1, 768), (60, 1, 896), (60, 1, 1024), (60, 1, 1152), (60, 1, 1189), (64, 1, 256), (64, 1, 384), (64, 1, 512), (64, 1, 640), (64, 1, 768), (64, 1, 896), (64, 1, 1024), (64, 1, 1152), (64, 1, 1189)]
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:52 [hpu_model_runner.py:5167] Multimodal bucket : [196, 256, 441, 480, 576, 900, 1156]
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/preprocessor_config.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m Image processor Qwen2VLImageProcessorFast {
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "crop_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "data_format": "channels_first",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "default_to_square": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "device": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "disable_grouping": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_center_crop": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_convert_rgb": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_normalize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_pad": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_rescale": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_resize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_mean": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_processor_type": "Qwen2VLImageProcessorFast",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_std": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "input_data_format": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "max_pixels": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "merge_size": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "min_pixels": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "pad_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "patch_size": 16,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "resample": 3,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "return_tensors": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "size": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "longest_edge": 16777216,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "shortest_edge": 65536
[0;36m(EngineCore_DP0 pid=65986)[0;0m   },
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "temporal_patch_size": 2
[0;36m(EngineCore_DP0 pid=65986)[0;0m }
[0;36m(EngineCore_DP0 pid=65986)[0;0m 
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file vocab.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file merges.txt
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file tokenizer.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file added_tokens.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file special_tokens_map.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file tokenizer_config.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file chat_template.jinja
[0;36m(EngineCore_DP0 pid=65986)[0;0m Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/video_preprocessor_config.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m Video processor Qwen3VLVideoProcessor {
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "crop_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "data_format": "channels_first",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "default_to_square": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "device": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_center_crop": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_convert_rgb": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_normalize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_rescale": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_resize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_sample_frames": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "fps": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_mean": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_std": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "input_data_format": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "max_frames": 768,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "merge_size": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "min_frames": 4,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "num_frames": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "pad_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "patch_size": 16,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "resample": 3,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "return_metadata": false,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "size": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "longest_edge": 25165824,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "shortest_edge": 4096
[0;36m(EngineCore_DP0 pid=65986)[0;0m   },
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "temporal_patch_size": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "video_metadata": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "video_processor_type": "Qwen3VLVideoProcessor"
[0;36m(EngineCore_DP0 pid=65986)[0;0m }
[0;36m(EngineCore_DP0 pid=65986)[0;0m 
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading configuration file None
[0;36m(EngineCore_DP0 pid=65986)[0;0m Processor Qwen3VLProcessor:
[0;36m(EngineCore_DP0 pid=65986)[0;0m - image_processor: Qwen2VLImageProcessorFast {
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "crop_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "data_format": "channels_first",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "default_to_square": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "device": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "disable_grouping": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_center_crop": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_convert_rgb": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_normalize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_pad": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_rescale": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_resize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_mean": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_processor_type": "Qwen2VLImageProcessorFast",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_std": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "input_data_format": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "max_pixels": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "merge_size": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "min_pixels": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "pad_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "patch_size": 16,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "resample": 3,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "return_tensors": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "size": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "longest_edge": 16777216,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "shortest_edge": 65536
[0;36m(EngineCore_DP0 pid=65986)[0;0m   },
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "temporal_patch_size": 2
[0;36m(EngineCore_DP0 pid=65986)[0;0m }
[0;36m(EngineCore_DP0 pid=65986)[0;0m 
[0;36m(EngineCore_DP0 pid=65986)[0;0m - tokenizer: Qwen2TokenizerFast(name_or_path='/root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/', vocab_size=151643, model_max_length=262144, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m }
[0;36m(EngineCore_DP0 pid=65986)[0;0m )
[0;36m(EngineCore_DP0 pid=65986)[0;0m - video_processor: Qwen3VLVideoProcessor {
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "crop_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "data_format": "channels_first",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "default_to_square": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "device": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_center_crop": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_convert_rgb": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_normalize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_rescale": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_resize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_sample_frames": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "fps": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_mean": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_std": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "input_data_format": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "max_frames": 768,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "merge_size": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "min_frames": 4,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "num_frames": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "pad_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "patch_size": 16,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "resample": 3,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "return_metadata": false,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "size": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "longest_edge": 25165824,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "shortest_edge": 4096
[0;36m(EngineCore_DP0 pid=65986)[0;0m   },
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "temporal_patch_size": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "video_metadata": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "video_processor_type": "Qwen3VLVideoProcessor"
[0;36m(EngineCore_DP0 pid=65986)[0;0m }
[0;36m(EngineCore_DP0 pid=65986)[0;0m 
[0;36m(EngineCore_DP0 pid=65986)[0;0m 
[0;36m(EngineCore_DP0 pid=65986)[0;0m {
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "processor_class": "Qwen3VLProcessor"
[0;36m(EngineCore_DP0 pid=65986)[0;0m }
[0;36m(EngineCore_DP0 pid=65986)[0;0m 
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/preprocessor_config.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m Image processor Qwen2VLImageProcessorFast {
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "crop_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "data_format": "channels_first",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "default_to_square": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "device": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "disable_grouping": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_center_crop": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_convert_rgb": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_normalize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_pad": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_rescale": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_resize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_mean": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_processor_type": "Qwen2VLImageProcessorFast",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_std": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "input_data_format": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "max_pixels": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "merge_size": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "min_pixels": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "pad_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "patch_size": 16,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "resample": 3,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "return_tensors": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "size": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "longest_edge": 1048576,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "shortest_edge": 65536
[0;36m(EngineCore_DP0 pid=65986)[0;0m   },
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "temporal_patch_size": 2
[0;36m(EngineCore_DP0 pid=65986)[0;0m }
[0;36m(EngineCore_DP0 pid=65986)[0;0m 
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file vocab.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file merges.txt
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file tokenizer.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file added_tokens.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file special_tokens_map.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file tokenizer_config.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading file chat_template.jinja
[0;36m(EngineCore_DP0 pid=65986)[0;0m Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/video_preprocessor_config.json
[0;36m(EngineCore_DP0 pid=65986)[0;0m Video processor Qwen3VLVideoProcessor {
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "crop_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "data_format": "channels_first",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "default_to_square": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "device": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_center_crop": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_convert_rgb": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_normalize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_rescale": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_resize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_sample_frames": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "fps": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_mean": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_std": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "input_data_format": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "max_frames": 768,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "merge_size": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "min_frames": 4,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "num_frames": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "pad_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "patch_size": 16,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "resample": 3,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "return_metadata": false,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "size": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "longest_edge": 1048576,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "shortest_edge": 65536
[0;36m(EngineCore_DP0 pid=65986)[0;0m   },
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "temporal_patch_size": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "video_metadata": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "video_processor_type": "Qwen3VLVideoProcessor"
[0;36m(EngineCore_DP0 pid=65986)[0;0m }
[0;36m(EngineCore_DP0 pid=65986)[0;0m 
[0;36m(EngineCore_DP0 pid=65986)[0;0m loading configuration file None
[0;36m(EngineCore_DP0 pid=65986)[0;0m Processor Qwen3VLProcessor:
[0;36m(EngineCore_DP0 pid=65986)[0;0m - image_processor: Qwen2VLImageProcessorFast {
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "crop_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "data_format": "channels_first",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "default_to_square": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "device": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "disable_grouping": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_center_crop": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_convert_rgb": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_normalize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_pad": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_rescale": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_resize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_mean": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_processor_type": "Qwen2VLImageProcessorFast",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_std": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "input_data_format": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "max_pixels": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "merge_size": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "min_pixels": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "pad_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "patch_size": 16,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "resample": 3,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "return_tensors": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "size": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "longest_edge": 1048576,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "shortest_edge": 65536
[0;36m(EngineCore_DP0 pid=65986)[0;0m   },
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "temporal_patch_size": 2
[0;36m(EngineCore_DP0 pid=65986)[0;0m }
[0;36m(EngineCore_DP0 pid=65986)[0;0m 
[0;36m(EngineCore_DP0 pid=65986)[0;0m - tokenizer: CachedQwen2TokenizerFast(name_or_path='/root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/', vocab_size=151643, model_max_length=262144, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m 	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(EngineCore_DP0 pid=65986)[0;0m }
[0;36m(EngineCore_DP0 pid=65986)[0;0m )
[0;36m(EngineCore_DP0 pid=65986)[0;0m - video_processor: Qwen3VLVideoProcessor {
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "crop_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "data_format": "channels_first",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "default_to_square": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "device": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_center_crop": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_convert_rgb": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_normalize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_rescale": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_resize": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "do_sample_frames": true,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "fps": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_mean": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "image_std": [
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     0.5
[0;36m(EngineCore_DP0 pid=65986)[0;0m   ],
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "input_data_format": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "max_frames": 768,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "merge_size": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "min_frames": 4,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "num_frames": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "pad_size": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "patch_size": 16,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "resample": 3,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "return_metadata": false,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "size": {
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "longest_edge": 1048576,
[0;36m(EngineCore_DP0 pid=65986)[0;0m     "shortest_edge": 65536
[0;36m(EngineCore_DP0 pid=65986)[0;0m   },
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "temporal_patch_size": 2,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "video_metadata": null,
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "video_processor_type": "Qwen3VLVideoProcessor"
[0;36m(EngineCore_DP0 pid=65986)[0;0m }
[0;36m(EngineCore_DP0 pid=65986)[0;0m 
[0;36m(EngineCore_DP0 pid=65986)[0;0m 
[0;36m(EngineCore_DP0 pid=65986)[0;0m {
[0;36m(EngineCore_DP0 pid=65986)[0;0m   "processor_class": "Qwen3VLProcessor"
[0;36m(EngineCore_DP0 pid=65986)[0;0m }
[0;36m(EngineCore_DP0 pid=65986)[0;0m 
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:58 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][1/36] batch_size:1 seq_len:0 resolution:864X480 free_mem:44.52 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:58 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][2/36] batch_size:1 seq_len:0 resolution:224X224 free_mem:44.32 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:59 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][3/36] batch_size:1 seq_len:0 resolution:288X224 free_mem:44.45 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:59 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][4/36] batch_size:1 seq_len:0 resolution:160X224 free_mem:44.66 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:44:59 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][5/36] batch_size:1 seq_len:0 resolution:384X224 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:00 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][6/36] batch_size:1 seq_len:0 resolution:128X224 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:00 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][7/36] batch_size:1 seq_len:0 resolution:256X256 free_mem:44.65 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:00 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][8/36] batch_size:1 seq_len:0 resolution:352X256 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:01 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][9/36] batch_size:1 seq_len:0 resolution:192X256 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:01 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][10/36] batch_size:1 seq_len:0 resolution:448X256 free_mem:44.59 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:01 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][11/36] batch_size:1 seq_len:0 resolution:160X256 free_mem:44.73 GiB
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:45:02 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:02 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][12/36] batch_size:1 seq_len:0 resolution:352X336 free_mem:43.93 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:02 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][13/36] batch_size:1 seq_len:0 resolution:448X336 free_mem:44.66 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:03 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][14/36] batch_size:1 seq_len:0 resolution:256X336 free_mem:44.32 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:03 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][15/36] batch_size:1 seq_len:0 resolution:608X336 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:03 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][16/36] batch_size:1 seq_len:0 resolution:192X336 free_mem:44.66 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:04 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][17/36] batch_size:1 seq_len:0 resolution:352X336 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:04 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][18/36] batch_size:1 seq_len:0 resolution:448X336 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:05 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][19/36] batch_size:1 seq_len:0 resolution:256X336 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:05 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][20/36] batch_size:1 seq_len:0 resolution:608X336 free_mem:44.32 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:05 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][21/36] batch_size:1 seq_len:0 resolution:192X336 free_mem:44.72 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:06 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][22/36] batch_size:1 seq_len:0 resolution:384X384 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:06 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][23/36] batch_size:1 seq_len:0 resolution:512X384 free_mem:44.45 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:06 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][24/36] batch_size:1 seq_len:0 resolution:288X384 free_mem:44.66 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:07 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][25/36] batch_size:1 seq_len:0 resolution:672X384 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:07 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][26/36] batch_size:1 seq_len:0 resolution:224X384 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:08 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][27/36] batch_size:1 seq_len:0 resolution:480X480 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:08 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][28/36] batch_size:1 seq_len:0 resolution:640X480 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:08 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][29/36] batch_size:1 seq_len:0 resolution:352X480 free_mem:44.45 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:09 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][30/36] batch_size:1 seq_len:0 resolution:864X480 free_mem:44.66 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:09 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][31/36] batch_size:1 seq_len:0 resolution:256X480 free_mem:44.66 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:09 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][32/36] batch_size:1 seq_len:0 resolution:544X544 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:10 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][33/36] batch_size:1 seq_len:0 resolution:736X544 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:10 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][34/36] batch_size:1 seq_len:0 resolution:416X544 free_mem:44.66 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:10 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][35/36] batch_size:1 seq_len:0 resolution:960X544 free_mem:44.73 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4311] [Warmup][Graph/Multimodal(image)][36/36] batch_size:1 seq_len:0 resolution:320X544 free_mem:44.66 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:45:11 [hpu_model_runner.py:5231] Using PT_COMPILE_ONLY_MODE.
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4439] Warming up sampler with batch sizes: [1, 2, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64] and following configs:
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4441] temp=0.0, top_p=1.0, top_k=0, batch_changed=True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4441] temp=1.0, top_p=1.0, top_k=0, batch_changed=True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4441] temp=0.7, top_p=0.9, top_k=50, batch_changed=True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4441] temp=0.3, top_p=0.95, top_k=20, batch_changed=True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4441] temp=1.2, top_p=0.8, top_k=100, batch_changed=True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4441] temp=0.8, top_p=0.85, top_k=0, batch_changed=True
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4441] temp=0.0, top_p=1.0, top_k=0, batch_changed=False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4441] temp=1.0, top_p=1.0, top_k=0, batch_changed=False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4441] temp=0.7, top_p=0.9, top_k=50, batch_changed=False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4441] temp=0.3, top_p=0.95, top_k=20, batch_changed=False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4441] temp=1.2, top_p=0.8, top_k=100, batch_changed=False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4441] temp=0.8, top_p=0.85, top_k=0, batch_changed=False
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:11 [hpu_model_runner.py:4442] Starting sampler warmup...
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:45:12 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:16 [hpu_model_runner.py:4531] Sampler warmup completed successfully
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:16 [hpu_model_runner.py:4548] Warming up defragmenter with thresholds: [8, 16, 32, 64, 128, 256, 512]
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:45:20 [hpu_model_runner.py:4572] Defragmenter warmup completed successfully
[0;36m(EngineCore_DP0 pid=65986)[0;0m Prompt warmup processing:   0%|          | 0/48 [00:00<?, ?item/s][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:45:22 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:   0%|          | 0/48 [00:11<?, ?item/s, 0/48]Prompt warmup processing:   2%|â–         | 1/48 [00:11<08:39, 11.06s/item, 0/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:45:32 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:   2%|â–         | 1/48 [00:16<08:39, 11.06s/item, 1/48]Prompt warmup processing:   4%|â–         | 2/48 [00:16<05:53,  7.69s/item, 1/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:45:42 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:   4%|â–         | 2/48 [00:25<05:53,  7.69s/item, 2/48]Prompt warmup processing:   6%|â–‹         | 3/48 [00:25<06:17,  8.39s/item, 2/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:45:52 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:   6%|â–‹         | 3/48 [00:34<06:17,  8.39s/item, 3/48]Prompt warmup processing:   8%|â–Š         | 4/48 [00:34<06:16,  8.55s/item, 3/48]Prompt warmup processing:   8%|â–Š         | 4/48 [00:36<06:16,  8.55s/item, 4/48]Prompt warmup processing:  10%|â–ˆ         | 5/48 [00:36<04:23,  6.13s/item, 4/48]Prompt warmup processing:  10%|â–ˆ         | 5/48 [00:40<04:23,  6.13s/item, 5/48]Prompt warmup processing:  12%|â–ˆâ–Ž        | 6/48 [00:40<03:56,  5.64s/item, 5/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:46:02 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:  12%|â–ˆâ–Ž        | 6/48 [00:46<03:56,  5.64s/item, 6/48]Prompt warmup processing:  15%|â–ˆâ–        | 7/48 [00:46<03:48,  5.57s/item, 6/48]Prompt warmup processing:  15%|â–ˆâ–        | 7/48 [00:48<03:48,  5.57s/item, 7/48]Prompt warmup processing:  17%|â–ˆâ–‹        | 8/48 [00:48<02:56,  4.42s/item, 7/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:46:12 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:  17%|â–ˆâ–‹        | 8/48 [00:52<02:56,  4.42s/item, 8/48]Prompt warmup processing:  19%|â–ˆâ–‰        | 9/48 [00:52<02:46,  4.28s/item, 8/48]Prompt warmup processing:  19%|â–ˆâ–‰        | 9/48 [00:56<02:46,  4.28s/item, 9/48]Prompt warmup processing:  21%|â–ˆâ–ˆ        | 10/48 [00:56<02:39,  4.20s/item, 9/48]Prompt warmup processing:  21%|â–ˆâ–ˆ        | 10/48 [00:57<02:39,  4.20s/item, 10/48]Prompt warmup processing:  23%|â–ˆâ–ˆâ–Ž       | 11/48 [00:57<02:06,  3.43s/item, 10/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:46:22 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:  23%|â–ˆâ–ˆâ–Ž       | 11/48 [01:02<02:06,  3.43s/item, 11/48]Prompt warmup processing:  25%|â–ˆâ–ˆâ–Œ       | 12/48 [01:02<02:11,  3.66s/item, 11/48]Prompt warmup processing:  25%|â–ˆâ–ˆâ–Œ       | 12/48 [01:07<02:11,  3.66s/item, 12/48]Prompt warmup processing:  27%|â–ˆâ–ˆâ–‹       | 13/48 [01:07<02:20,  4.02s/item, 12/48]Prompt warmup processing:  27%|â–ˆâ–ˆâ–‹       | 13/48 [01:09<02:20,  4.02s/item, 13/48]Prompt warmup processing:  29%|â–ˆâ–ˆâ–‰       | 14/48 [01:09<01:58,  3.50s/item, 13/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:46:32 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:  29%|â–ˆâ–ˆâ–‰       | 14/48 [01:14<01:58,  3.50s/item, 14/48]Prompt warmup processing:  31%|â–ˆâ–ˆâ–ˆâ–      | 15/48 [01:14<02:13,  4.05s/item, 14/48]Prompt warmup processing:  31%|â–ˆâ–ˆâ–ˆâ–      | 15/48 [01:17<02:13,  4.05s/item, 15/48]Prompt warmup processing:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 16/48 [01:17<01:54,  3.58s/item, 15/48]Prompt warmup processing:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 16/48 [01:19<01:54,  3.58s/item, 16/48]Prompt warmup processing:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 17/48 [01:19<01:40,  3.23s/item, 16/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:46:42 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 17/48 [01:23<01:40,  3.23s/item, 17/48]Prompt warmup processing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 18/48 [01:23<01:44,  3.50s/item, 17/48]Prompt warmup processing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 18/48 [01:26<01:44,  3.50s/item, 18/48]Prompt warmup processing:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 19/48 [01:26<01:33,  3.23s/item, 18/48]Prompt warmup processing:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 19/48 [01:28<01:33,  3.23s/item, 19/48]Prompt warmup processing:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20/48 [01:28<01:22,  2.94s/item, 19/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:46:52 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20/48 [01:32<01:22,  2.94s/item, 20/48]Prompt warmup processing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/48 [01:32<01:30,  3.35s/item, 20/48]Prompt warmup processing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/48 [01:35<01:30,  3.35s/item, 21/48]Prompt warmup processing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 22/48 [01:35<01:22,  3.16s/item, 21/48]Prompt warmup processing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 22/48 [01:37<01:22,  3.16s/item, 22/48]Prompt warmup processing:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23/48 [01:37<01:09,  2.79s/item, 22/48]Prompt warmup processing:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23/48 [01:40<01:09,  2.79s/item, 23/48]Prompt warmup processing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 24/48 [01:40<01:12,  3.01s/item, 23/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:47:02 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 24/48 [01:43<01:12,  3.01s/item, 24/48]Prompt warmup processing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 25/48 [01:43<01:03,  2.74s/item, 24/48]Prompt warmup processing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 25/48 [01:46<01:03,  2.74s/item, 25/48]Prompt warmup processing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/48 [01:46<01:02,  2.82s/item, 25/48]Prompt warmup processing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/48 [01:49<01:02,  2.82s/item, 26/48]Prompt warmup processing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 27/48 [01:49<01:03,  3.04s/item, 26/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:47:12 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 27/48 [01:51<01:03,  3.04s/item, 27/48]Prompt warmup processing:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 28/48 [01:51<00:55,  2.78s/item, 27/48]Prompt warmup processing:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 28/48 [01:53<00:55,  2.78s/item, 28/48]Prompt warmup processing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 29/48 [01:53<00:48,  2.57s/item, 28/48]Prompt warmup processing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 29/48 [01:56<00:48,  2.57s/item, 29/48]Prompt warmup processing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 30/48 [01:56<00:46,  2.59s/item, 29/48]Prompt warmup processing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 30/48 [01:58<00:46,  2.59s/item, 30/48]Prompt warmup processing:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/48 [01:58<00:42,  2.50s/item, 30/48]Prompt warmup processing:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/48 [02:00<00:42,  2.50s/item, 31/48]Prompt warmup processing:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 32/48 [02:00<00:37,  2.33s/item, 31/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:47:22 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 32/48 [02:03<00:37,  2.33s/item, 32/48]Prompt warmup processing:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 33/48 [02:03<00:38,  2.56s/item, 32/48]Prompt warmup processing:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 33/48 [02:06<00:38,  2.56s/item, 33/48]Prompt warmup processing:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 34/48 [02:06<00:34,  2.49s/item, 33/48]Prompt warmup processing:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 34/48 [02:08<00:34,  2.49s/item, 34/48]Prompt warmup processing:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 35/48 [02:08<00:30,  2.32s/item, 34/48]Prompt warmup processing:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 35/48 [02:10<00:30,  2.32s/item, 35/48]Prompt warmup processing:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 36/48 [02:10<00:27,  2.31s/item, 35/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:47:32 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 36/48 [02:12<00:27,  2.31s/item, 36/48]Prompt warmup processing:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 37/48 [02:12<00:25,  2.29s/item, 36/48]Prompt warmup processing:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 37/48 [02:14<00:25,  2.29s/item, 37/48]Prompt warmup processing:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 38/48 [02:14<00:21,  2.17s/item, 37/48]Prompt warmup processing:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 38/48 [02:16<00:21,  2.17s/item, 38/48]Prompt warmup processing:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 39/48 [02:16<00:19,  2.19s/item, 38/48]Prompt warmup processing:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 39/48 [02:19<00:19,  2.19s/item, 39/48]Prompt warmup processing:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 40/48 [02:19<00:18,  2.28s/item, 39/48]Prompt warmup processing:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 40/48 [02:21<00:18,  2.28s/item, 40/48]Prompt warmup processing:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 41/48 [02:21<00:15,  2.22s/item, 40/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:47:42 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 41/48 [02:24<00:15,  2.22s/item, 41/48]Prompt warmup processing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 42/48 [02:24<00:14,  2.35s/item, 41/48]Prompt warmup processing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 42/48 [02:27<00:14,  2.35s/item, 42/48]Prompt warmup processing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 43/48 [02:27<00:13,  2.63s/item, 42/48]Prompt warmup processing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 43/48 [02:30<00:13,  2.63s/item, 43/48]Prompt warmup processing:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 44/48 [02:30<00:10,  2.68s/item, 43/48][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:47:52 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Prompt warmup processing:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 44/48 [02:32<00:10,  2.68s/item, 44/48]Prompt warmup processing:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45/48 [02:32<00:08,  2.70s/item, 44/48]Prompt warmup processing:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45/48 [02:35<00:08,  2.70s/item, 45/48]Prompt warmup processing:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 46/48 [02:35<00:05,  2.72s/item, 45/48]Prompt warmup processing:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 46/48 [02:37<00:05,  2.72s/item, 46/48]Prompt warmup processing:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 47/48 [02:37<00:02,  2.58s/item, 46/48]Prompt warmup processing:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 47/48 [02:40<00:02,  2.58s/item, 47/48]Prompt warmup processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [02:40<00:00,  2.53s/item, 47/48]Prompt warmup processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [02:40<00:00,  3.34s/item, 47/48]
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:48:00 [hpu_model_runner.py:4285] Graph/Prompt captured:48 used_mem:11.11 GiB
[0;36m(EngineCore_DP0 pid=65986)[0;0m Decode warmup processing:   0%|          | 0/162 [00:00<?, ?item/s][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:48:02 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:   0%|          | 0/162 [00:05<?, ?item/s, 0/162]Decode warmup processing:   1%|          | 1/162 [00:05<13:59,  5.21s/item, 0/162]Decode warmup processing:   1%|          | 1/162 [00:10<13:59,  5.21s/item, 1/162]Decode warmup processing:   1%|          | 2/162 [00:10<14:38,  5.49s/item, 1/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:48:12 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:   1%|          | 2/162 [00:12<14:38,  5.49s/item, 2/162]Decode warmup processing:   2%|â–         | 3/162 [00:12<09:43,  3.67s/item, 2/162]Decode warmup processing:   2%|â–         | 3/162 [00:13<09:43,  3.67s/item, 3/162]Decode warmup processing:   2%|â–         | 4/162 [00:13<07:15,  2.76s/item, 3/162]Decode warmup processing:   2%|â–         | 4/162 [00:15<07:15,  2.76s/item, 4/162]Decode warmup processing:   3%|â–Ž         | 5/162 [00:15<05:56,  2.27s/item, 4/162]Decode warmup processing:   3%|â–Ž         | 5/162 [00:16<05:56,  2.27s/item, 5/162]Decode warmup processing:   4%|â–Ž         | 6/162 [00:16<05:08,  1.98s/item, 5/162]Decode warmup processing:   4%|â–Ž         | 6/162 [00:18<05:08,  1.98s/item, 6/162]Decode warmup processing:   4%|â–         | 7/162 [00:18<04:39,  1.80s/item, 6/162]Decode warmup processing:   4%|â–         | 7/162 [00:19<04:39,  1.80s/item, 7/162]Decode warmup processing:   5%|â–         | 8/162 [00:19<04:10,  1.62s/item, 7/162]Decode warmup processing:   5%|â–         | 8/162 [00:20<04:10,  1.62s/item, 8/162]Decode warmup processing:   6%|â–Œ         | 9/162 [00:20<03:52,  1.52s/item, 8/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:48:22 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:   6%|â–Œ         | 9/162 [00:27<03:52,  1.52s/item, 9/162]Decode warmup processing:   6%|â–Œ         | 10/162 [00:27<08:22,  3.31s/item, 9/162]Decode warmup processing:   6%|â–Œ         | 10/162 [00:29<08:22,  3.31s/item, 10/162]Decode warmup processing:   7%|â–‹         | 11/162 [00:29<06:44,  2.68s/item, 10/162]Decode warmup processing:   7%|â–‹         | 11/162 [00:30<06:44,  2.68s/item, 11/162]Decode warmup processing:   7%|â–‹         | 12/162 [00:30<05:44,  2.30s/item, 11/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:48:32 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:   7%|â–‹         | 12/162 [00:31<05:44,  2.30s/item, 12/162]Decode warmup processing:   8%|â–Š         | 13/162 [00:31<04:55,  1.98s/item, 12/162]Decode warmup processing:   8%|â–Š         | 13/162 [00:32<04:55,  1.98s/item, 13/162]Decode warmup processing:   9%|â–Š         | 14/162 [00:32<04:18,  1.74s/item, 13/162]Decode warmup processing:   9%|â–Š         | 14/162 [00:34<04:18,  1.74s/item, 14/162]Decode warmup processing:   9%|â–‰         | 15/162 [00:34<03:58,  1.62s/item, 14/162]Decode warmup processing:   9%|â–‰         | 15/162 [00:35<03:58,  1.62s/item, 15/162]Decode warmup processing:  10%|â–‰         | 16/162 [00:35<03:43,  1.53s/item, 15/162]Decode warmup processing:  10%|â–‰         | 16/162 [00:36<03:43,  1.53s/item, 16/162]Decode warmup processing:  10%|â–ˆ         | 17/162 [00:36<03:28,  1.44s/item, 16/162]Decode warmup processing:  10%|â–ˆ         | 17/162 [00:38<03:28,  1.44s/item, 17/162]Decode warmup processing:  11%|â–ˆ         | 18/162 [00:38<03:25,  1.43s/item, 17/162]Decode warmup processing:  11%|â–ˆ         | 18/162 [00:39<03:25,  1.43s/item, 18/162]Decode warmup processing:  12%|â–ˆâ–        | 19/162 [00:39<03:20,  1.40s/item, 18/162]Decode warmup processing:  12%|â–ˆâ–        | 19/162 [00:41<03:20,  1.40s/item, 19/162]Decode warmup processing:  12%|â–ˆâ–        | 20/162 [00:41<03:18,  1.40s/item, 19/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:48:42 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  12%|â–ˆâ–        | 20/162 [00:42<03:18,  1.40s/item, 20/162]Decode warmup processing:  13%|â–ˆâ–Ž        | 21/162 [00:42<03:19,  1.42s/item, 20/162]Decode warmup processing:  13%|â–ˆâ–Ž        | 21/162 [00:44<03:19,  1.42s/item, 21/162]Decode warmup processing:  14%|â–ˆâ–Ž        | 22/162 [00:44<03:22,  1.45s/item, 21/162]Decode warmup processing:  14%|â–ˆâ–Ž        | 22/162 [00:45<03:22,  1.45s/item, 22/162]Decode warmup processing:  14%|â–ˆâ–        | 23/162 [00:45<03:17,  1.42s/item, 22/162]Decode warmup processing:  14%|â–ˆâ–        | 23/162 [00:46<03:17,  1.42s/item, 23/162]Decode warmup processing:  15%|â–ˆâ–        | 24/162 [00:46<03:09,  1.38s/item, 23/162]Decode warmup processing:  15%|â–ˆâ–        | 24/162 [00:48<03:09,  1.38s/item, 24/162]Decode warmup processing:  15%|â–ˆâ–Œ        | 25/162 [00:48<03:09,  1.39s/item, 24/162]Decode warmup processing:  15%|â–ˆâ–Œ        | 25/162 [00:49<03:09,  1.39s/item, 25/162]Decode warmup processing:  16%|â–ˆâ–Œ        | 26/162 [00:49<03:06,  1.37s/item, 25/162]Decode warmup processing:  16%|â–ˆâ–Œ        | 26/162 [00:50<03:06,  1.37s/item, 26/162]Decode warmup processing:  17%|â–ˆâ–‹        | 27/162 [00:50<03:03,  1.36s/item, 26/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:48:52 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  17%|â–ˆâ–‹        | 27/162 [00:52<03:03,  1.36s/item, 27/162]Decode warmup processing:  17%|â–ˆâ–‹        | 28/162 [00:52<03:04,  1.38s/item, 27/162]Decode warmup processing:  17%|â–ˆâ–‹        | 28/162 [00:53<03:04,  1.38s/item, 28/162]Decode warmup processing:  18%|â–ˆâ–Š        | 29/162 [00:53<03:08,  1.42s/item, 28/162]Decode warmup processing:  18%|â–ˆâ–Š        | 29/162 [00:55<03:08,  1.42s/item, 29/162]Decode warmup processing:  19%|â–ˆâ–Š        | 30/162 [00:55<03:09,  1.44s/item, 29/162]Decode warmup processing:  19%|â–ˆâ–Š        | 30/162 [00:56<03:09,  1.44s/item, 30/162]Decode warmup processing:  19%|â–ˆâ–‰        | 31/162 [00:56<03:08,  1.44s/item, 30/162]Decode warmup processing:  19%|â–ˆâ–‰        | 31/162 [00:57<03:08,  1.44s/item, 31/162]Decode warmup processing:  20%|â–ˆâ–‰        | 32/162 [00:57<03:05,  1.43s/item, 31/162]Decode warmup processing:  20%|â–ˆâ–‰        | 32/162 [00:59<03:05,  1.43s/item, 32/162]Decode warmup processing:  20%|â–ˆâ–ˆ        | 33/162 [00:59<03:01,  1.40s/item, 32/162]Decode warmup processing:  20%|â–ˆâ–ˆ        | 33/162 [01:00<03:01,  1.40s/item, 33/162]Decode warmup processing:  21%|â–ˆâ–ˆ        | 34/162 [01:00<02:57,  1.38s/item, 33/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:49:02 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  21%|â–ˆâ–ˆ        | 34/162 [01:01<02:57,  1.38s/item, 34/162]Decode warmup processing:  22%|â–ˆâ–ˆâ–       | 35/162 [01:01<02:49,  1.33s/item, 34/162]Decode warmup processing:  22%|â–ˆâ–ˆâ–       | 35/162 [01:03<02:49,  1.33s/item, 35/162]Decode warmup processing:  22%|â–ˆâ–ˆâ–       | 36/162 [01:03<02:44,  1.30s/item, 35/162]Decode warmup processing:  22%|â–ˆâ–ˆâ–       | 36/162 [01:04<02:44,  1.30s/item, 36/162]Decode warmup processing:  23%|â–ˆâ–ˆâ–Ž       | 37/162 [01:04<02:53,  1.39s/item, 36/162]Decode warmup processing:  23%|â–ˆâ–ˆâ–Ž       | 37/162 [01:05<02:53,  1.39s/item, 37/162]Decode warmup processing:  23%|â–ˆâ–ˆâ–Ž       | 38/162 [01:05<02:47,  1.35s/item, 37/162]Decode warmup processing:  23%|â–ˆâ–ˆâ–Ž       | 38/162 [01:07<02:47,  1.35s/item, 38/162]Decode warmup processing:  24%|â–ˆâ–ˆâ–       | 39/162 [01:07<02:46,  1.36s/item, 38/162]Decode warmup processing:  24%|â–ˆâ–ˆâ–       | 39/162 [01:08<02:46,  1.36s/item, 39/162]Decode warmup processing:  25%|â–ˆâ–ˆâ–       | 40/162 [01:08<02:47,  1.38s/item, 39/162]Decode warmup processing:  25%|â–ˆâ–ˆâ–       | 40/162 [01:10<02:47,  1.38s/item, 40/162]Decode warmup processing:  25%|â–ˆâ–ˆâ–Œ       | 41/162 [01:10<02:44,  1.36s/item, 40/162]Decode warmup processing:  25%|â–ˆâ–ˆâ–Œ       | 41/162 [01:11<02:44,  1.36s/item, 41/162]Decode warmup processing:  26%|â–ˆâ–ˆâ–Œ       | 42/162 [01:11<02:43,  1.36s/item, 41/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:49:12 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  26%|â–ˆâ–ˆâ–Œ       | 42/162 [01:12<02:43,  1.36s/item, 42/162]Decode warmup processing:  27%|â–ˆâ–ˆâ–‹       | 43/162 [01:12<02:43,  1.38s/item, 42/162]Decode warmup processing:  27%|â–ˆâ–ˆâ–‹       | 43/162 [01:14<02:43,  1.38s/item, 43/162]Decode warmup processing:  27%|â–ˆâ–ˆâ–‹       | 44/162 [01:14<02:40,  1.36s/item, 43/162]Decode warmup processing:  27%|â–ˆâ–ˆâ–‹       | 44/162 [01:15<02:40,  1.36s/item, 44/162]Decode warmup processing:  28%|â–ˆâ–ˆâ–Š       | 45/162 [01:15<02:42,  1.39s/item, 44/162]Decode warmup processing:  28%|â–ˆâ–ˆâ–Š       | 45/162 [01:17<02:42,  1.39s/item, 45/162]Decode warmup processing:  28%|â–ˆâ–ˆâ–Š       | 46/162 [01:17<02:44,  1.42s/item, 45/162]Decode warmup processing:  28%|â–ˆâ–ˆâ–Š       | 46/162 [01:18<02:44,  1.42s/item, 46/162]Decode warmup processing:  29%|â–ˆâ–ˆâ–‰       | 47/162 [01:18<02:38,  1.38s/item, 46/162]Decode warmup processing:  29%|â–ˆâ–ˆâ–‰       | 47/162 [01:19<02:38,  1.38s/item, 47/162]Decode warmup processing:  30%|â–ˆâ–ˆâ–‰       | 48/162 [01:19<02:36,  1.37s/item, 47/162]Decode warmup processing:  30%|â–ˆâ–ˆâ–‰       | 48/162 [01:21<02:36,  1.37s/item, 48/162]Decode warmup processing:  30%|â–ˆâ–ˆâ–ˆ       | 49/162 [01:21<02:38,  1.40s/item, 48/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:49:22 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  30%|â–ˆâ–ˆâ–ˆ       | 49/162 [01:22<02:38,  1.40s/item, 49/162]Decode warmup processing:  31%|â–ˆâ–ˆâ–ˆ       | 50/162 [01:22<02:37,  1.41s/item, 49/162]Decode warmup processing:  31%|â–ˆâ–ˆâ–ˆ       | 50/162 [01:23<02:37,  1.41s/item, 50/162]Decode warmup processing:  31%|â–ˆâ–ˆâ–ˆâ–      | 51/162 [01:23<02:34,  1.39s/item, 50/162]Decode warmup processing:  31%|â–ˆâ–ˆâ–ˆâ–      | 51/162 [01:25<02:34,  1.39s/item, 51/162]Decode warmup processing:  32%|â–ˆâ–ˆâ–ˆâ–      | 52/162 [01:25<02:29,  1.36s/item, 51/162]Decode warmup processing:  32%|â–ˆâ–ˆâ–ˆâ–      | 52/162 [01:26<02:29,  1.36s/item, 52/162]Decode warmup processing:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 53/162 [01:26<02:28,  1.36s/item, 52/162]Decode warmup processing:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 53/162 [01:28<02:28,  1.36s/item, 53/162]Decode warmup processing:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 54/162 [01:28<02:28,  1.38s/item, 53/162]Decode warmup processing:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 54/162 [01:29<02:28,  1.38s/item, 54/162]Decode warmup processing:  34%|â–ˆâ–ˆâ–ˆâ–      | 55/162 [01:29<02:32,  1.42s/item, 54/162]Decode warmup processing:  34%|â–ˆâ–ˆâ–ˆâ–      | 55/162 [01:31<02:32,  1.42s/item, 55/162]Decode warmup processing:  35%|â–ˆâ–ˆâ–ˆâ–      | 56/162 [01:31<02:33,  1.45s/item, 55/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:49:32 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  35%|â–ˆâ–ˆâ–ˆâ–      | 56/162 [01:32<02:33,  1.45s/item, 56/162]Decode warmup processing:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 57/162 [01:32<02:36,  1.49s/item, 56/162]Decode warmup processing:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 57/162 [01:33<02:36,  1.49s/item, 57/162]Decode warmup processing:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 58/162 [01:33<02:29,  1.44s/item, 57/162]Decode warmup processing:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 58/162 [01:35<02:29,  1.44s/item, 58/162]Decode warmup processing:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 59/162 [01:35<02:24,  1.40s/item, 58/162]Decode warmup processing:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 59/162 [01:36<02:24,  1.40s/item, 59/162]Decode warmup processing:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 60/162 [01:36<02:20,  1.38s/item, 59/162]Decode warmup processing:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 60/162 [01:37<02:20,  1.38s/item, 60/162]Decode warmup processing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 61/162 [01:37<02:15,  1.35s/item, 60/162]Decode warmup processing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 61/162 [01:39<02:15,  1.35s/item, 61/162]Decode warmup processing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 62/162 [01:39<02:14,  1.34s/item, 61/162]Decode warmup processing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 62/162 [01:40<02:14,  1.34s/item, 62/162]Decode warmup processing:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 63/162 [01:40<02:11,  1.33s/item, 62/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:49:42 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 63/162 [01:41<02:11,  1.33s/item, 63/162]Decode warmup processing:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 64/162 [01:41<02:13,  1.36s/item, 63/162]Decode warmup processing:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 64/162 [01:43<02:13,  1.36s/item, 64/162]Decode warmup processing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 65/162 [01:43<02:15,  1.40s/item, 64/162]Decode warmup processing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 65/162 [01:44<02:15,  1.40s/item, 65/162]Decode warmup processing:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 66/162 [01:44<02:14,  1.40s/item, 65/162]Decode warmup processing:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 66/162 [01:46<02:14,  1.40s/item, 66/162]Decode warmup processing:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 67/162 [01:46<02:12,  1.39s/item, 66/162]Decode warmup processing:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 67/162 [01:47<02:12,  1.39s/item, 67/162]Decode warmup processing:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 68/162 [01:47<02:10,  1.39s/item, 67/162]Decode warmup processing:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 68/162 [01:49<02:10,  1.39s/item, 68/162]Decode warmup processing:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 69/162 [01:49<02:10,  1.40s/item, 68/162]Decode warmup processing:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 69/162 [01:50<02:10,  1.40s/item, 69/162]Decode warmup processing:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 70/162 [01:50<02:11,  1.43s/item, 69/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:49:52 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 70/162 [01:51<02:11,  1.43s/item, 70/162]Decode warmup processing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 71/162 [01:51<02:03,  1.36s/item, 70/162]Decode warmup processing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 71/162 [01:52<02:03,  1.36s/item, 71/162]Decode warmup processing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 72/162 [01:52<01:57,  1.30s/item, 71/162]Decode warmup processing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 72/162 [01:54<01:57,  1.30s/item, 72/162]Decode warmup processing:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 73/162 [01:54<01:55,  1.29s/item, 72/162]Decode warmup processing:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 73/162 [01:55<01:55,  1.29s/item, 73/162]Decode warmup processing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 74/162 [01:55<01:56,  1.33s/item, 73/162]Decode warmup processing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 74/162 [01:57<01:56,  1.33s/item, 74/162]Decode warmup processing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 75/162 [01:57<01:58,  1.36s/item, 74/162]Decode warmup processing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 75/162 [01:58<01:58,  1.36s/item, 75/162]Decode warmup processing:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 76/162 [01:58<02:00,  1.41s/item, 75/162]Decode warmup processing:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 76/162 [01:59<02:00,  1.41s/item, 76/162]Decode warmup processing:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 77/162 [01:59<01:59,  1.41s/item, 76/162]Decode warmup processing:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 77/162 [02:01<01:59,  1.41s/item, 77/162]Decode warmup processing:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 78/162 [02:01<01:55,  1.38s/item, 77/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:50:02 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 78/162 [02:02<01:55,  1.38s/item, 78/162]Decode warmup processing:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 79/162 [02:02<01:55,  1.39s/item, 78/162]Decode warmup processing:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 79/162 [02:04<01:55,  1.39s/item, 79/162]Decode warmup processing:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 80/162 [02:04<01:55,  1.41s/item, 79/162]Decode warmup processing:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 80/162 [02:05<01:55,  1.41s/item, 80/162]Decode warmup processing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 81/162 [02:05<01:50,  1.37s/item, 80/162]Decode warmup processing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 81/162 [02:06<01:50,  1.37s/item, 81/162]Decode warmup processing:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 82/162 [02:06<01:52,  1.41s/item, 81/162]Decode warmup processing:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 82/162 [02:08<01:52,  1.41s/item, 82/162]Decode warmup processing:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 83/162 [02:08<01:48,  1.37s/item, 82/162]Decode warmup processing:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 83/162 [02:09<01:48,  1.37s/item, 83/162]Decode warmup processing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 84/162 [02:09<01:44,  1.34s/item, 83/162]Decode warmup processing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 84/162 [02:10<01:44,  1.34s/item, 84/162]Decode warmup processing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 85/162 [02:10<01:40,  1.31s/item, 84/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:50:12 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 85/162 [02:12<01:40,  1.31s/item, 85/162]Decode warmup processing:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 86/162 [02:12<01:40,  1.33s/item, 85/162]Decode warmup processing:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 86/162 [02:13<01:40,  1.33s/item, 86/162]Decode warmup processing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 87/162 [02:13<01:37,  1.30s/item, 86/162]Decode warmup processing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 87/162 [02:14<01:37,  1.30s/item, 87/162]Decode warmup processing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 88/162 [02:14<01:38,  1.33s/item, 87/162]Decode warmup processing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 88/162 [02:16<01:38,  1.33s/item, 88/162]Decode warmup processing:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 89/162 [02:16<01:38,  1.35s/item, 88/162]Decode warmup processing:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 89/162 [02:17<01:38,  1.35s/item, 89/162]Decode warmup processing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 90/162 [02:17<01:38,  1.37s/item, 89/162]Decode warmup processing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 90/162 [02:18<01:38,  1.37s/item, 90/162]Decode warmup processing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 91/162 [02:18<01:37,  1.38s/item, 90/162]Decode warmup processing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 91/162 [02:20<01:37,  1.38s/item, 91/162]Decode warmup processing:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 92/162 [02:20<01:33,  1.34s/item, 91/162]Decode warmup processing:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 92/162 [02:21<01:33,  1.34s/item, 92/162]Decode warmup processing:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 93/162 [02:21<01:32,  1.34s/item, 92/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:50:22 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 93/162 [02:22<01:32,  1.34s/item, 93/162]Decode warmup processing:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 94/162 [02:22<01:30,  1.33s/item, 93/162]Decode warmup processing:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 94/162 [02:24<01:30,  1.33s/item, 94/162]Decode warmup processing:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 95/162 [02:24<01:27,  1.31s/item, 94/162]Decode warmup processing:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 95/162 [02:25<01:27,  1.31s/item, 95/162]Decode warmup processing:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 96/162 [02:25<01:25,  1.29s/item, 95/162]Decode warmup processing:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 96/162 [02:26<01:25,  1.29s/item, 96/162]Decode warmup processing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 97/162 [02:26<01:28,  1.36s/item, 96/162]Decode warmup processing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 97/162 [02:28<01:28,  1.36s/item, 97/162]Decode warmup processing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 98/162 [02:28<01:27,  1.36s/item, 97/162]Decode warmup processing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 98/162 [02:29<01:27,  1.36s/item, 98/162]Decode warmup processing:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 99/162 [02:29<01:25,  1.36s/item, 98/162]Decode warmup processing:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 99/162 [02:30<01:25,  1.36s/item, 99/162]Decode warmup processing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 100/162 [02:30<01:24,  1.36s/item, 99/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:50:32 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 100/162 [02:32<01:24,  1.36s/item, 100/162]Decode warmup processing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 101/162 [02:32<01:21,  1.33s/item, 100/162]Decode warmup processing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 101/162 [02:33<01:21,  1.33s/item, 101/162]Decode warmup processing:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 102/162 [02:33<01:20,  1.34s/item, 101/162]Decode warmup processing:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 102/162 [02:34<01:20,  1.34s/item, 102/162]Decode warmup processing:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 103/162 [02:34<01:18,  1.34s/item, 102/162]Decode warmup processing:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 103/162 [02:36<01:18,  1.34s/item, 103/162]Decode warmup processing:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 104/162 [02:36<01:16,  1.31s/item, 103/162]Decode warmup processing:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 104/162 [02:37<01:16,  1.31s/item, 104/162]Decode warmup processing:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 105/162 [02:37<01:15,  1.33s/item, 104/162]Decode warmup processing:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 105/162 [02:38<01:15,  1.33s/item, 105/162]Decode warmup processing:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 106/162 [02:38<01:14,  1.34s/item, 105/162]Decode warmup processing:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 106/162 [02:40<01:14,  1.34s/item, 106/162]Decode warmup processing:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 107/162 [02:40<01:14,  1.36s/item, 106/162]Decode warmup processing:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 107/162 [02:41<01:14,  1.36s/item, 107/162]Decode warmup processing:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 108/162 [02:41<01:10,  1.31s/item, 107/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:50:42 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 108/162 [02:42<01:10,  1.31s/item, 108/162]Decode warmup processing:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 109/162 [02:42<01:13,  1.38s/item, 108/162]Decode warmup processing:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 109/162 [02:44<01:13,  1.38s/item, 109/162]Decode warmup processing:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 110/162 [02:44<01:13,  1.42s/item, 109/162]Decode warmup processing:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 110/162 [02:46<01:13,  1.42s/item, 110/162]Decode warmup processing:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 111/162 [02:46<01:14,  1.46s/item, 110/162]Decode warmup processing:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 111/162 [02:47<01:14,  1.46s/item, 111/162]Decode warmup processing:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 112/162 [02:47<01:13,  1.47s/item, 111/162]Decode warmup processing:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 112/162 [02:48<01:13,  1.47s/item, 112/162]Decode warmup processing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 113/162 [02:48<01:10,  1.44s/item, 112/162]Decode warmup processing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 113/162 [02:50<01:10,  1.44s/item, 113/162]Decode warmup processing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 114/162 [02:50<01:07,  1.41s/item, 113/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:50:52 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 114/162 [02:51<01:07,  1.41s/item, 114/162]Decode warmup processing:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 115/162 [02:51<01:06,  1.42s/item, 114/162]Decode warmup processing:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 115/162 [02:53<01:06,  1.42s/item, 115/162]Decode warmup processing:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 116/162 [02:53<01:04,  1.39s/item, 115/162]Decode warmup processing:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 116/162 [02:54<01:04,  1.39s/item, 116/162]Decode warmup processing:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 117/162 [02:54<01:00,  1.35s/item, 116/162]Decode warmup processing:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 117/162 [02:55<01:00,  1.35s/item, 117/162]Decode warmup processing:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 118/162 [02:55<00:58,  1.34s/item, 117/162]Decode warmup processing:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 118/162 [02:56<00:58,  1.34s/item, 118/162]Decode warmup processing:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 119/162 [02:56<00:56,  1.31s/item, 118/162]Decode warmup processing:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 119/162 [02:58<00:56,  1.31s/item, 119/162]Decode warmup processing:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 120/162 [02:58<00:54,  1.31s/item, 119/162]Decode warmup processing:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 120/162 [02:59<00:54,  1.31s/item, 120/162]Decode warmup processing:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 121/162 [02:59<00:55,  1.35s/item, 120/162]Decode warmup processing:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 121/162 [03:00<00:55,  1.35s/item, 121/162]Decode warmup processing:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 122/162 [03:00<00:53,  1.35s/item, 121/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:51:02 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 122/162 [03:02<00:53,  1.35s/item, 122/162]Decode warmup processing:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 123/162 [03:02<00:51,  1.32s/item, 122/162]Decode warmup processing:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 123/162 [03:03<00:51,  1.32s/item, 123/162]Decode warmup processing:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 124/162 [03:03<00:49,  1.30s/item, 123/162]Decode warmup processing:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 124/162 [03:04<00:49,  1.30s/item, 124/162]Decode warmup processing:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 125/162 [03:04<00:48,  1.32s/item, 124/162]Decode warmup processing:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 125/162 [03:06<00:48,  1.32s/item, 125/162]Decode warmup processing:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 126/162 [03:06<00:48,  1.36s/item, 125/162]Decode warmup processing:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 126/162 [03:07<00:48,  1.36s/item, 126/162]Decode warmup processing:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 127/162 [03:07<00:50,  1.44s/item, 126/162]Decode warmup processing:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 127/162 [03:09<00:50,  1.44s/item, 127/162]Decode warmup processing:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 128/162 [03:09<00:49,  1.46s/item, 127/162]Decode warmup processing:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 128/162 [03:10<00:49,  1.46s/item, 128/162]Decode warmup processing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 129/162 [03:10<00:46,  1.41s/item, 128/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:51:12 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 129/162 [03:12<00:46,  1.41s/item, 129/162]Decode warmup processing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 130/162 [03:12<00:44,  1.40s/item, 129/162]Decode warmup processing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 130/162 [03:13<00:44,  1.40s/item, 130/162]Decode warmup processing:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 131/162 [03:13<00:42,  1.38s/item, 130/162]Decode warmup processing:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 131/162 [03:14<00:42,  1.38s/item, 131/162]Decode warmup processing:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 132/162 [03:14<00:40,  1.36s/item, 131/162]Decode warmup processing:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 132/162 [03:16<00:40,  1.36s/item, 132/162]Decode warmup processing:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 133/162 [03:16<00:39,  1.37s/item, 132/162]Decode warmup processing:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 133/162 [03:17<00:39,  1.37s/item, 133/162]Decode warmup processing:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 134/162 [03:17<00:37,  1.35s/item, 133/162]Decode warmup processing:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 134/162 [03:18<00:37,  1.35s/item, 134/162]Decode warmup processing:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 135/162 [03:18<00:35,  1.31s/item, 134/162]Decode warmup processing:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 135/162 [03:20<00:35,  1.31s/item, 135/162]Decode warmup processing:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 136/162 [03:20<00:35,  1.38s/item, 135/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:51:22 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 136/162 [03:21<00:35,  1.38s/item, 136/162]Decode warmup processing:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 137/162 [03:21<00:36,  1.44s/item, 136/162]Decode warmup processing:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 137/162 [03:23<00:36,  1.44s/item, 137/162]Decode warmup processing:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 138/162 [03:23<00:34,  1.44s/item, 137/162]Decode warmup processing:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 138/162 [03:24<00:34,  1.44s/item, 138/162]Decode warmup processing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 139/162 [03:24<00:32,  1.40s/item, 138/162]Decode warmup processing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 139/162 [03:25<00:32,  1.40s/item, 139/162]Decode warmup processing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 140/162 [03:25<00:30,  1.37s/item, 139/162]Decode warmup processing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 140/162 [03:27<00:30,  1.37s/item, 140/162]Decode warmup processing:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 141/162 [03:27<00:28,  1.35s/item, 140/162]Decode warmup processing:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 141/162 [03:28<00:28,  1.35s/item, 141/162]Decode warmup processing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 142/162 [03:28<00:27,  1.38s/item, 141/162]Decode warmup processing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 142/162 [03:29<00:27,  1.38s/item, 142/162]Decode warmup processing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 143/162 [03:29<00:26,  1.37s/item, 142/162]Decode warmup processing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 143/162 [03:31<00:26,  1.37s/item, 143/162]Decode warmup processing:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 144/162 [03:31<00:23,  1.32s/item, 143/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:51:32 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 144/162 [03:32<00:23,  1.32s/item, 144/162]Decode warmup processing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 145/162 [03:32<00:23,  1.38s/item, 144/162]Decode warmup processing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 145/162 [03:33<00:23,  1.38s/item, 145/162]Decode warmup processing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 146/162 [03:33<00:21,  1.36s/item, 145/162]Decode warmup processing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 146/162 [03:35<00:21,  1.36s/item, 146/162]Decode warmup processing:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 147/162 [03:35<00:20,  1.35s/item, 146/162]Decode warmup processing:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 147/162 [03:36<00:20,  1.35s/item, 147/162]Decode warmup processing:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/162 [03:36<00:19,  1.37s/item, 147/162]Decode warmup processing:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/162 [03:37<00:19,  1.37s/item, 148/162]Decode warmup processing:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 149/162 [03:37<00:17,  1.35s/item, 148/162]Decode warmup processing:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 149/162 [03:39<00:17,  1.35s/item, 149/162]Decode warmup processing:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 150/162 [03:39<00:15,  1.30s/item, 149/162]Decode warmup processing:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 150/162 [03:40<00:15,  1.30s/item, 150/162]Decode warmup processing:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 151/162 [03:40<00:13,  1.27s/item, 150/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:51:42 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 151/162 [03:41<00:13,  1.27s/item, 151/162]Decode warmup processing:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 152/162 [03:41<00:13,  1.31s/item, 151/162]Decode warmup processing:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 152/162 [03:43<00:13,  1.31s/item, 152/162]Decode warmup processing:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 153/162 [03:43<00:11,  1.30s/item, 152/162]Decode warmup processing:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 153/162 [03:49<00:11,  1.30s/item, 153/162]Decode warmup processing:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 154/162 [03:49<00:22,  2.82s/item, 153/162]Decode warmup processing:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 154/162 [03:50<00:22,  2.82s/item, 154/162]Decode warmup processing:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 155/162 [03:50<00:16,  2.37s/item, 154/162][0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:51:52 [v1/engine/utils.py:982] Waiting for 1 local, 0 remote core engine proc(s) to start.
Decode warmup processing:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 155/162 [03:52<00:16,  2.37s/item, 155/162]Decode warmup processing:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 156/162 [03:52<00:12,  2.06s/item, 155/162]Decode warmup processing:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 156/162 [03:53<00:12,  2.06s/item, 156/162]Decode warmup processing:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 157/162 [03:53<00:09,  1.88s/item, 156/162]Decode warmup processing:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 157/162 [03:54<00:09,  1.88s/item, 157/162]Decode warmup processing:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 158/162 [03:54<00:06,  1.71s/item, 157/162]Decode warmup processing:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 158/162 [03:56<00:06,  1.71s/item, 158/162]Decode warmup processing:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 159/162 [03:56<00:04,  1.61s/item, 158/162]Decode warmup processing:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 159/162 [03:57<00:04,  1.61s/item, 159/162]Decode warmup processing:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 160/162 [03:57<00:03,  1.57s/item, 159/162]Decode warmup processing:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 160/162 [03:59<00:03,  1.57s/item, 160/162]Decode warmup processing:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 161/162 [03:59<00:01,  1.54s/item, 160/162]Decode warmup processing:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 161/162 [04:00<00:01,  1.54s/item, 161/162]Decode warmup processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 162/162 [04:00<00:00,  1.52s/item, 161/162]Decode warmup processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 162/162 [04:00<00:00,  1.49s/item, 161/162]
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:52:01 [hpu_model_runner.py:4285] Graph/Decode captured:162 used_mem:0 B
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:52:01 [hpu_model_runner.py:5272] Warmup finished in 429 secs, allocated 11.11 GiB of device memory
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:52:01 [v1/engine/core.py:272] init engine (profile, create kv cache, warmup model) took 467.97 seconds
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:52:01 [v1/engine/core.py:185] Batch queue is enabled with size 2
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:52:02 [utils/gc_utils.py:40] GC Debug Config. enabled:False,top_objects:-1
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:52:02 [v1/engine/utils.py:1093] READY from local core engine process 0.
[0;36m(EngineCore_DP0 pid=65986)[0;0m INFO 02-09 19:52:02 [config/vllm.py:633] Asynchronous scheduling is enabled.
[0;36m(EngineCore_DP0 pid=65986)[0;0m WARNING 02-09 19:52:02 [config/vllm.py:678] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=65986)[0;0m WARNING 02-09 19:52:02 [platform.py:143] Using Contiguous PA, disabling prefix caching
[0;36m(EngineCore_DP0 pid=65986)[0;0m =========compilation_config.custom_ops=['all']===========
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:52:02 [v1/engine/core.py:982] EngineCore waiting for work.
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:52:02 [v1/metrics/loggers.py:271] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 1189
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:52:02 [v1/engine/core.py:982] EngineCore waiting for work.
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:52:02 [v1/engine/core.py:982] EngineCore waiting for work.
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/openai/api_server.py:443] Supported tasks: ['generate']
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/generation_config.json
[0;36m(APIServer pid=65602)[0;0m Generate config GenerationConfig {
[0;36m(APIServer pid=65602)[0;0m   "bos_token_id": 151643,
[0;36m(APIServer pid=65602)[0;0m   "do_sample": true,
[0;36m(APIServer pid=65602)[0;0m   "eos_token_id": [
[0;36m(APIServer pid=65602)[0;0m     151645,
[0;36m(APIServer pid=65602)[0;0m     151643
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "pad_token_id": 151643,
[0;36m(APIServer pid=65602)[0;0m   "temperature": 0.7,
[0;36m(APIServer pid=65602)[0;0m   "top_k": 20,
[0;36m(APIServer pid=65602)[0;0m   "top_p": 0.8
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m WARNING 02-09 19:52:02 [config/model.py:1368] Default vLLM sampling parameters have been overridden by the model's `generation_config.json`: `{'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}`. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/generation_config.json
[0;36m(APIServer pid=65602)[0;0m Generate config GenerationConfig {
[0;36m(APIServer pid=65602)[0;0m   "bos_token_id": 151643,
[0;36m(APIServer pid=65602)[0;0m   "do_sample": true,
[0;36m(APIServer pid=65602)[0;0m   "eos_token_id": [
[0;36m(APIServer pid=65602)[0;0m     151645,
[0;36m(APIServer pid=65602)[0;0m     151643
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "pad_token_id": 151643,
[0;36m(APIServer pid=65602)[0;0m   "temperature": 0.7,
[0;36m(APIServer pid=65602)[0;0m   "top_k": 20,
[0;36m(APIServer pid=65602)[0;0m   "top_p": 0.8
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/.../chat_completion/serving.py:184] Warming up chat template processing...
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/preprocessor_config.json
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/preprocessor_config.json
[0;36m(APIServer pid=65602)[0;0m Image processor Qwen2VLImageProcessorFast {
[0;36m(APIServer pid=65602)[0;0m   "crop_size": null,
[0;36m(APIServer pid=65602)[0;0m   "data_format": "channels_first",
[0;36m(APIServer pid=65602)[0;0m   "default_to_square": true,
[0;36m(APIServer pid=65602)[0;0m   "device": null,
[0;36m(APIServer pid=65602)[0;0m   "disable_grouping": null,
[0;36m(APIServer pid=65602)[0;0m   "do_center_crop": null,
[0;36m(APIServer pid=65602)[0;0m   "do_convert_rgb": true,
[0;36m(APIServer pid=65602)[0;0m   "do_normalize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_pad": null,
[0;36m(APIServer pid=65602)[0;0m   "do_rescale": true,
[0;36m(APIServer pid=65602)[0;0m   "do_resize": true,
[0;36m(APIServer pid=65602)[0;0m   "image_mean": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "image_processor_type": "Qwen2VLImageProcessorFast",
[0;36m(APIServer pid=65602)[0;0m   "image_std": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "input_data_format": null,
[0;36m(APIServer pid=65602)[0;0m   "max_pixels": null,
[0;36m(APIServer pid=65602)[0;0m   "merge_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "min_pixels": null,
[0;36m(APIServer pid=65602)[0;0m   "pad_size": null,
[0;36m(APIServer pid=65602)[0;0m   "patch_size": 16,
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(APIServer pid=65602)[0;0m   "resample": 3,
[0;36m(APIServer pid=65602)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(APIServer pid=65602)[0;0m   "return_tensors": null,
[0;36m(APIServer pid=65602)[0;0m   "size": {
[0;36m(APIServer pid=65602)[0;0m     "longest_edge": 16777216,
[0;36m(APIServer pid=65602)[0;0m     "shortest_edge": 65536
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "temporal_patch_size": 2
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m loading file vocab.json
[0;36m(APIServer pid=65602)[0;0m loading file merges.txt
[0;36m(APIServer pid=65602)[0;0m loading file tokenizer.json
[0;36m(APIServer pid=65602)[0;0m loading file added_tokens.json
[0;36m(APIServer pid=65602)[0;0m loading file special_tokens_map.json
[0;36m(APIServer pid=65602)[0;0m loading file tokenizer_config.json
[0;36m(APIServer pid=65602)[0;0m loading file chat_template.jinja
[0;36m(APIServer pid=65602)[0;0m Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/video_preprocessor_config.json
[0;36m(APIServer pid=65602)[0;0m Video processor Qwen3VLVideoProcessor {
[0;36m(APIServer pid=65602)[0;0m   "crop_size": null,
[0;36m(APIServer pid=65602)[0;0m   "data_format": "channels_first",
[0;36m(APIServer pid=65602)[0;0m   "default_to_square": true,
[0;36m(APIServer pid=65602)[0;0m   "device": null,
[0;36m(APIServer pid=65602)[0;0m   "do_center_crop": null,
[0;36m(APIServer pid=65602)[0;0m   "do_convert_rgb": true,
[0;36m(APIServer pid=65602)[0;0m   "do_normalize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_rescale": true,
[0;36m(APIServer pid=65602)[0;0m   "do_resize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_sample_frames": true,
[0;36m(APIServer pid=65602)[0;0m   "fps": 2,
[0;36m(APIServer pid=65602)[0;0m   "image_mean": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "image_std": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "input_data_format": null,
[0;36m(APIServer pid=65602)[0;0m   "max_frames": 768,
[0;36m(APIServer pid=65602)[0;0m   "merge_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "min_frames": 4,
[0;36m(APIServer pid=65602)[0;0m   "num_frames": null,
[0;36m(APIServer pid=65602)[0;0m   "pad_size": null,
[0;36m(APIServer pid=65602)[0;0m   "patch_size": 16,
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(APIServer pid=65602)[0;0m   "resample": 3,
[0;36m(APIServer pid=65602)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(APIServer pid=65602)[0;0m   "return_metadata": false,
[0;36m(APIServer pid=65602)[0;0m   "size": {
[0;36m(APIServer pid=65602)[0;0m     "longest_edge": 25165824,
[0;36m(APIServer pid=65602)[0;0m     "shortest_edge": 4096
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "temporal_patch_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "video_metadata": null,
[0;36m(APIServer pid=65602)[0;0m   "video_processor_type": "Qwen3VLVideoProcessor"
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m loading configuration file None
[0;36m(APIServer pid=65602)[0;0m Processor Qwen3VLProcessor:
[0;36m(APIServer pid=65602)[0;0m - image_processor: Qwen2VLImageProcessorFast {
[0;36m(APIServer pid=65602)[0;0m   "crop_size": null,
[0;36m(APIServer pid=65602)[0;0m   "data_format": "channels_first",
[0;36m(APIServer pid=65602)[0;0m   "default_to_square": true,
[0;36m(APIServer pid=65602)[0;0m   "device": null,
[0;36m(APIServer pid=65602)[0;0m   "disable_grouping": null,
[0;36m(APIServer pid=65602)[0;0m   "do_center_crop": null,
[0;36m(APIServer pid=65602)[0;0m   "do_convert_rgb": true,
[0;36m(APIServer pid=65602)[0;0m   "do_normalize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_pad": null,
[0;36m(APIServer pid=65602)[0;0m   "do_rescale": true,
[0;36m(APIServer pid=65602)[0;0m   "do_resize": true,
[0;36m(APIServer pid=65602)[0;0m   "image_mean": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "image_processor_type": "Qwen2VLImageProcessorFast",
[0;36m(APIServer pid=65602)[0;0m   "image_std": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "input_data_format": null,
[0;36m(APIServer pid=65602)[0;0m   "max_pixels": null,
[0;36m(APIServer pid=65602)[0;0m   "merge_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "min_pixels": null,
[0;36m(APIServer pid=65602)[0;0m   "pad_size": null,
[0;36m(APIServer pid=65602)[0;0m   "patch_size": 16,
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(APIServer pid=65602)[0;0m   "resample": 3,
[0;36m(APIServer pid=65602)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(APIServer pid=65602)[0;0m   "return_tensors": null,
[0;36m(APIServer pid=65602)[0;0m   "size": {
[0;36m(APIServer pid=65602)[0;0m     "longest_edge": 16777216,
[0;36m(APIServer pid=65602)[0;0m     "shortest_edge": 65536
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "temporal_patch_size": 2
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m - tokenizer: Qwen2TokenizerFast(name_or_path='/root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/', vocab_size=151643, model_max_length=262144, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
[0;36m(APIServer pid=65602)[0;0m 	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m )
[0;36m(APIServer pid=65602)[0;0m - video_processor: Qwen3VLVideoProcessor {
[0;36m(APIServer pid=65602)[0;0m   "crop_size": null,
[0;36m(APIServer pid=65602)[0;0m   "data_format": "channels_first",
[0;36m(APIServer pid=65602)[0;0m   "default_to_square": true,
[0;36m(APIServer pid=65602)[0;0m   "device": null,
[0;36m(APIServer pid=65602)[0;0m   "do_center_crop": null,
[0;36m(APIServer pid=65602)[0;0m   "do_convert_rgb": true,
[0;36m(APIServer pid=65602)[0;0m   "do_normalize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_rescale": true,
[0;36m(APIServer pid=65602)[0;0m   "do_resize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_sample_frames": true,
[0;36m(APIServer pid=65602)[0;0m   "fps": 2,
[0;36m(APIServer pid=65602)[0;0m   "image_mean": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "image_std": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "input_data_format": null,
[0;36m(APIServer pid=65602)[0;0m   "max_frames": 768,
[0;36m(APIServer pid=65602)[0;0m   "merge_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "min_frames": 4,
[0;36m(APIServer pid=65602)[0;0m   "num_frames": null,
[0;36m(APIServer pid=65602)[0;0m   "pad_size": null,
[0;36m(APIServer pid=65602)[0;0m   "patch_size": 16,
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(APIServer pid=65602)[0;0m   "resample": 3,
[0;36m(APIServer pid=65602)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(APIServer pid=65602)[0;0m   "return_metadata": false,
[0;36m(APIServer pid=65602)[0;0m   "size": {
[0;36m(APIServer pid=65602)[0;0m     "longest_edge": 25165824,
[0;36m(APIServer pid=65602)[0;0m     "shortest_edge": 4096
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "temporal_patch_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "video_metadata": null,
[0;36m(APIServer pid=65602)[0;0m   "video_processor_type": "Qwen3VLVideoProcessor"
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m {
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor"
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [renderers/hf.py:316] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/.../chat_completion/serving.py:219] Chat template warmup completed in 597.5ms
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/generation_config.json
[0;36m(APIServer pid=65602)[0;0m Generate config GenerationConfig {
[0;36m(APIServer pid=65602)[0;0m   "bos_token_id": 151643,
[0;36m(APIServer pid=65602)[0;0m   "do_sample": true,
[0;36m(APIServer pid=65602)[0;0m   "eos_token_id": [
[0;36m(APIServer pid=65602)[0;0m     151645,
[0;36m(APIServer pid=65602)[0;0m     151643
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "pad_token_id": 151643,
[0;36m(APIServer pid=65602)[0;0m   "temperature": 0.7,
[0;36m(APIServer pid=65602)[0;0m   "top_k": 20,
[0;36m(APIServer pid=65602)[0;0m   "top_p": 0.8
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/generation_config.json
[0;36m(APIServer pid=65602)[0;0m Generate config GenerationConfig {
[0;36m(APIServer pid=65602)[0;0m   "bos_token_id": 151643,
[0;36m(APIServer pid=65602)[0;0m   "do_sample": true,
[0;36m(APIServer pid=65602)[0;0m   "eos_token_id": [
[0;36m(APIServer pid=65602)[0;0m     151645,
[0;36m(APIServer pid=65602)[0;0m     151643
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "pad_token_id": 151643,
[0;36m(APIServer pid=65602)[0;0m   "temperature": 0.7,
[0;36m(APIServer pid=65602)[0;0m   "top_k": 20,
[0;36m(APIServer pid=65602)[0;0m   "top_p": 0.8
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/openai/api_server.py:448] Starting vLLM API server 0 on http://localhost:12346
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:38] Available routes are:
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /v1/chat/completions/render, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /v1/completions/render, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:52:02 [entrypoints/launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=65602)[0;0m INFO:     Started server process [65602]
[0;36m(APIServer pid=65602)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=65602)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:52:13 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:52:23 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:52:33 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:52:43 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:52:53 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:53:03 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:53:13 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:53:23 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:53:33 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:53:43 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:53:53 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:54:03 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:54:13 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:54:23 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:54:33 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:54:43 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:54:53 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:55:03 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:55:13 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:55:23 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:55:33 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:55:43 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:55:53 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:56:03 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:56:13 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:56:23 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:56:33 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:56:43 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:56:53 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:57:03 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40596 - "GET /metrics HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:57:10 [v1/sample/logits_processor/__init__.py:63] No logitsprocs plugins installed (group vllm.logits_processors).
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:57:10 [v1/engine/input_processor.py:539] OMP_NUM_THREADS is not set; defaulting Torch threads to 1 for input preprocessing.
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/preprocessor_config.json
[0;36m(APIServer pid=65602)[0;0m Image processor Qwen2VLImageProcessorFast {
[0;36m(APIServer pid=65602)[0;0m   "crop_size": null,
[0;36m(APIServer pid=65602)[0;0m   "data_format": "channels_first",
[0;36m(APIServer pid=65602)[0;0m   "default_to_square": true,
[0;36m(APIServer pid=65602)[0;0m   "device": null,
[0;36m(APIServer pid=65602)[0;0m   "disable_grouping": null,
[0;36m(APIServer pid=65602)[0;0m   "do_center_crop": null,
[0;36m(APIServer pid=65602)[0;0m   "do_convert_rgb": true,
[0;36m(APIServer pid=65602)[0;0m   "do_normalize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_pad": null,
[0;36m(APIServer pid=65602)[0;0m   "do_rescale": true,
[0;36m(APIServer pid=65602)[0;0m   "do_resize": true,
[0;36m(APIServer pid=65602)[0;0m   "image_mean": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "image_processor_type": "Qwen2VLImageProcessorFast",
[0;36m(APIServer pid=65602)[0;0m   "image_std": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "input_data_format": null,
[0;36m(APIServer pid=65602)[0;0m   "max_pixels": null,
[0;36m(APIServer pid=65602)[0;0m   "merge_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "min_pixels": null,
[0;36m(APIServer pid=65602)[0;0m   "pad_size": null,
[0;36m(APIServer pid=65602)[0;0m   "patch_size": 16,
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(APIServer pid=65602)[0;0m   "resample": 3,
[0;36m(APIServer pid=65602)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(APIServer pid=65602)[0;0m   "return_tensors": null,
[0;36m(APIServer pid=65602)[0;0m   "size": {
[0;36m(APIServer pid=65602)[0;0m     "longest_edge": 16777216,
[0;36m(APIServer pid=65602)[0;0m     "shortest_edge": 65536
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "temporal_patch_size": 2
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m loading file vocab.json
[0;36m(APIServer pid=65602)[0;0m loading file merges.txt
[0;36m(APIServer pid=65602)[0;0m loading file tokenizer.json
[0;36m(APIServer pid=65602)[0;0m loading file added_tokens.json
[0;36m(APIServer pid=65602)[0;0m loading file special_tokens_map.json
[0;36m(APIServer pid=65602)[0;0m loading file tokenizer_config.json
[0;36m(APIServer pid=65602)[0;0m loading file chat_template.jinja
[0;36m(APIServer pid=65602)[0;0m Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/video_preprocessor_config.json
[0;36m(APIServer pid=65602)[0;0m Video processor Qwen3VLVideoProcessor {
[0;36m(APIServer pid=65602)[0;0m   "crop_size": null,
[0;36m(APIServer pid=65602)[0;0m   "data_format": "channels_first",
[0;36m(APIServer pid=65602)[0;0m   "default_to_square": true,
[0;36m(APIServer pid=65602)[0;0m   "device": null,
[0;36m(APIServer pid=65602)[0;0m   "do_center_crop": null,
[0;36m(APIServer pid=65602)[0;0m   "do_convert_rgb": true,
[0;36m(APIServer pid=65602)[0;0m   "do_normalize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_rescale": true,
[0;36m(APIServer pid=65602)[0;0m   "do_resize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_sample_frames": true,
[0;36m(APIServer pid=65602)[0;0m   "fps": 2,
[0;36m(APIServer pid=65602)[0;0m   "image_mean": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "image_std": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "input_data_format": null,
[0;36m(APIServer pid=65602)[0;0m   "max_frames": 768,
[0;36m(APIServer pid=65602)[0;0m   "merge_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "min_frames": 4,
[0;36m(APIServer pid=65602)[0;0m   "num_frames": null,
[0;36m(APIServer pid=65602)[0;0m   "pad_size": null,
[0;36m(APIServer pid=65602)[0;0m   "patch_size": 16,
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(APIServer pid=65602)[0;0m   "resample": 3,
[0;36m(APIServer pid=65602)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(APIServer pid=65602)[0;0m   "return_metadata": false,
[0;36m(APIServer pid=65602)[0;0m   "size": {
[0;36m(APIServer pid=65602)[0;0m     "longest_edge": 25165824,
[0;36m(APIServer pid=65602)[0;0m     "shortest_edge": 4096
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "temporal_patch_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "video_metadata": null,
[0;36m(APIServer pid=65602)[0;0m   "video_processor_type": "Qwen3VLVideoProcessor"
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m loading configuration file None
[0;36m(APIServer pid=65602)[0;0m Processor Qwen3VLProcessor:
[0;36m(APIServer pid=65602)[0;0m - image_processor: Qwen2VLImageProcessorFast {
[0;36m(APIServer pid=65602)[0;0m   "crop_size": null,
[0;36m(APIServer pid=65602)[0;0m   "data_format": "channels_first",
[0;36m(APIServer pid=65602)[0;0m   "default_to_square": true,
[0;36m(APIServer pid=65602)[0;0m   "device": null,
[0;36m(APIServer pid=65602)[0;0m   "disable_grouping": null,
[0;36m(APIServer pid=65602)[0;0m   "do_center_crop": null,
[0;36m(APIServer pid=65602)[0;0m   "do_convert_rgb": true,
[0;36m(APIServer pid=65602)[0;0m   "do_normalize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_pad": null,
[0;36m(APIServer pid=65602)[0;0m   "do_rescale": true,
[0;36m(APIServer pid=65602)[0;0m   "do_resize": true,
[0;36m(APIServer pid=65602)[0;0m   "image_mean": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "image_processor_type": "Qwen2VLImageProcessorFast",
[0;36m(APIServer pid=65602)[0;0m   "image_std": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "input_data_format": null,
[0;36m(APIServer pid=65602)[0;0m   "max_pixels": null,
[0;36m(APIServer pid=65602)[0;0m   "merge_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "min_pixels": null,
[0;36m(APIServer pid=65602)[0;0m   "pad_size": null,
[0;36m(APIServer pid=65602)[0;0m   "patch_size": 16,
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(APIServer pid=65602)[0;0m   "resample": 3,
[0;36m(APIServer pid=65602)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(APIServer pid=65602)[0;0m   "return_tensors": null,
[0;36m(APIServer pid=65602)[0;0m   "size": {
[0;36m(APIServer pid=65602)[0;0m     "longest_edge": 16777216,
[0;36m(APIServer pid=65602)[0;0m     "shortest_edge": 65536
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "temporal_patch_size": 2
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m - tokenizer: Qwen2TokenizerFast(name_or_path='/root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/', vocab_size=151643, model_max_length=262144, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
[0;36m(APIServer pid=65602)[0;0m 	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m )
[0;36m(APIServer pid=65602)[0;0m - video_processor: Qwen3VLVideoProcessor {
[0;36m(APIServer pid=65602)[0;0m   "crop_size": null,
[0;36m(APIServer pid=65602)[0;0m   "data_format": "channels_first",
[0;36m(APIServer pid=65602)[0;0m   "default_to_square": true,
[0;36m(APIServer pid=65602)[0;0m   "device": null,
[0;36m(APIServer pid=65602)[0;0m   "do_center_crop": null,
[0;36m(APIServer pid=65602)[0;0m   "do_convert_rgb": true,
[0;36m(APIServer pid=65602)[0;0m   "do_normalize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_rescale": true,
[0;36m(APIServer pid=65602)[0;0m   "do_resize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_sample_frames": true,
[0;36m(APIServer pid=65602)[0;0m   "fps": 2,
[0;36m(APIServer pid=65602)[0;0m   "image_mean": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "image_std": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "input_data_format": null,
[0;36m(APIServer pid=65602)[0;0m   "max_frames": 768,
[0;36m(APIServer pid=65602)[0;0m   "merge_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "min_frames": 4,
[0;36m(APIServer pid=65602)[0;0m   "num_frames": null,
[0;36m(APIServer pid=65602)[0;0m   "pad_size": null,
[0;36m(APIServer pid=65602)[0;0m   "patch_size": 16,
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(APIServer pid=65602)[0;0m   "resample": 3,
[0;36m(APIServer pid=65602)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(APIServer pid=65602)[0;0m   "return_metadata": false,
[0;36m(APIServer pid=65602)[0;0m   "size": {
[0;36m(APIServer pid=65602)[0;0m     "longest_edge": 25165824,
[0;36m(APIServer pid=65602)[0;0m     "shortest_edge": 4096
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "temporal_patch_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "video_metadata": null,
[0;36m(APIServer pid=65602)[0;0m   "video_processor_type": "Qwen3VLVideoProcessor"
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m {
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor"
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/preprocessor_config.json
[0;36m(APIServer pid=65602)[0;0m Image processor Qwen2VLImageProcessorFast {
[0;36m(APIServer pid=65602)[0;0m   "crop_size": null,
[0;36m(APIServer pid=65602)[0;0m   "data_format": "channels_first",
[0;36m(APIServer pid=65602)[0;0m   "default_to_square": true,
[0;36m(APIServer pid=65602)[0;0m   "device": null,
[0;36m(APIServer pid=65602)[0;0m   "disable_grouping": null,
[0;36m(APIServer pid=65602)[0;0m   "do_center_crop": null,
[0;36m(APIServer pid=65602)[0;0m   "do_convert_rgb": true,
[0;36m(APIServer pid=65602)[0;0m   "do_normalize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_pad": null,
[0;36m(APIServer pid=65602)[0;0m   "do_rescale": true,
[0;36m(APIServer pid=65602)[0;0m   "do_resize": true,
[0;36m(APIServer pid=65602)[0;0m   "image_mean": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "image_processor_type": "Qwen2VLImageProcessorFast",
[0;36m(APIServer pid=65602)[0;0m   "image_std": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "input_data_format": null,
[0;36m(APIServer pid=65602)[0;0m   "max_pixels": null,
[0;36m(APIServer pid=65602)[0;0m   "merge_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "min_pixels": null,
[0;36m(APIServer pid=65602)[0;0m   "pad_size": null,
[0;36m(APIServer pid=65602)[0;0m   "patch_size": 16,
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(APIServer pid=65602)[0;0m   "resample": 3,
[0;36m(APIServer pid=65602)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(APIServer pid=65602)[0;0m   "return_tensors": null,
[0;36m(APIServer pid=65602)[0;0m   "size": {
[0;36m(APIServer pid=65602)[0;0m     "longest_edge": 1048576,
[0;36m(APIServer pid=65602)[0;0m     "shortest_edge": 65536
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "temporal_patch_size": 2
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m loading file vocab.json
[0;36m(APIServer pid=65602)[0;0m loading file merges.txt
[0;36m(APIServer pid=65602)[0;0m loading file tokenizer.json
[0;36m(APIServer pid=65602)[0;0m loading file added_tokens.json
[0;36m(APIServer pid=65602)[0;0m loading file special_tokens_map.json
[0;36m(APIServer pid=65602)[0;0m loading file tokenizer_config.json
[0;36m(APIServer pid=65602)[0;0m loading file chat_template.jinja
[0;36m(APIServer pid=65602)[0;0m Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[0;36m(APIServer pid=65602)[0;0m loading configuration file /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/video_preprocessor_config.json
[0;36m(APIServer pid=65602)[0;0m Video processor Qwen3VLVideoProcessor {
[0;36m(APIServer pid=65602)[0;0m   "crop_size": null,
[0;36m(APIServer pid=65602)[0;0m   "data_format": "channels_first",
[0;36m(APIServer pid=65602)[0;0m   "default_to_square": true,
[0;36m(APIServer pid=65602)[0;0m   "device": null,
[0;36m(APIServer pid=65602)[0;0m   "do_center_crop": null,
[0;36m(APIServer pid=65602)[0;0m   "do_convert_rgb": true,
[0;36m(APIServer pid=65602)[0;0m   "do_normalize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_rescale": true,
[0;36m(APIServer pid=65602)[0;0m   "do_resize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_sample_frames": true,
[0;36m(APIServer pid=65602)[0;0m   "fps": 2,
[0;36m(APIServer pid=65602)[0;0m   "image_mean": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "image_std": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "input_data_format": null,
[0;36m(APIServer pid=65602)[0;0m   "max_frames": 768,
[0;36m(APIServer pid=65602)[0;0m   "merge_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "min_frames": 4,
[0;36m(APIServer pid=65602)[0;0m   "num_frames": null,
[0;36m(APIServer pid=65602)[0;0m   "pad_size": null,
[0;36m(APIServer pid=65602)[0;0m   "patch_size": 16,
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(APIServer pid=65602)[0;0m   "resample": 3,
[0;36m(APIServer pid=65602)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(APIServer pid=65602)[0;0m   "return_metadata": false,
[0;36m(APIServer pid=65602)[0;0m   "size": {
[0;36m(APIServer pid=65602)[0;0m     "longest_edge": 1048576,
[0;36m(APIServer pid=65602)[0;0m     "shortest_edge": 65536
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "temporal_patch_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "video_metadata": null,
[0;36m(APIServer pid=65602)[0;0m   "video_processor_type": "Qwen3VLVideoProcessor"
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m loading configuration file None
[0;36m(APIServer pid=65602)[0;0m Processor Qwen3VLProcessor:
[0;36m(APIServer pid=65602)[0;0m - image_processor: Qwen2VLImageProcessorFast {
[0;36m(APIServer pid=65602)[0;0m   "crop_size": null,
[0;36m(APIServer pid=65602)[0;0m   "data_format": "channels_first",
[0;36m(APIServer pid=65602)[0;0m   "default_to_square": true,
[0;36m(APIServer pid=65602)[0;0m   "device": null,
[0;36m(APIServer pid=65602)[0;0m   "disable_grouping": null,
[0;36m(APIServer pid=65602)[0;0m   "do_center_crop": null,
[0;36m(APIServer pid=65602)[0;0m   "do_convert_rgb": true,
[0;36m(APIServer pid=65602)[0;0m   "do_normalize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_pad": null,
[0;36m(APIServer pid=65602)[0;0m   "do_rescale": true,
[0;36m(APIServer pid=65602)[0;0m   "do_resize": true,
[0;36m(APIServer pid=65602)[0;0m   "image_mean": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "image_processor_type": "Qwen2VLImageProcessorFast",
[0;36m(APIServer pid=65602)[0;0m   "image_std": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "input_data_format": null,
[0;36m(APIServer pid=65602)[0;0m   "max_pixels": null,
[0;36m(APIServer pid=65602)[0;0m   "merge_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "min_pixels": null,
[0;36m(APIServer pid=65602)[0;0m   "pad_size": null,
[0;36m(APIServer pid=65602)[0;0m   "patch_size": 16,
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(APIServer pid=65602)[0;0m   "resample": 3,
[0;36m(APIServer pid=65602)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(APIServer pid=65602)[0;0m   "return_tensors": null,
[0;36m(APIServer pid=65602)[0;0m   "size": {
[0;36m(APIServer pid=65602)[0;0m     "longest_edge": 1048576,
[0;36m(APIServer pid=65602)[0;0m     "shortest_edge": 65536
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "temporal_patch_size": 2
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m - tokenizer: CachedQwen2TokenizerFast(name_or_path='/root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/', vocab_size=151643, model_max_length=262144, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
[0;36m(APIServer pid=65602)[0;0m 	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
[0;36m(APIServer pid=65602)[0;0m 	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m 	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m )
[0;36m(APIServer pid=65602)[0;0m - video_processor: Qwen3VLVideoProcessor {
[0;36m(APIServer pid=65602)[0;0m   "crop_size": null,
[0;36m(APIServer pid=65602)[0;0m   "data_format": "channels_first",
[0;36m(APIServer pid=65602)[0;0m   "default_to_square": true,
[0;36m(APIServer pid=65602)[0;0m   "device": null,
[0;36m(APIServer pid=65602)[0;0m   "do_center_crop": null,
[0;36m(APIServer pid=65602)[0;0m   "do_convert_rgb": true,
[0;36m(APIServer pid=65602)[0;0m   "do_normalize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_rescale": true,
[0;36m(APIServer pid=65602)[0;0m   "do_resize": true,
[0;36m(APIServer pid=65602)[0;0m   "do_sample_frames": true,
[0;36m(APIServer pid=65602)[0;0m   "fps": 2,
[0;36m(APIServer pid=65602)[0;0m   "image_mean": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "image_std": [
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5,
[0;36m(APIServer pid=65602)[0;0m     0.5
[0;36m(APIServer pid=65602)[0;0m   ],
[0;36m(APIServer pid=65602)[0;0m   "input_data_format": null,
[0;36m(APIServer pid=65602)[0;0m   "max_frames": 768,
[0;36m(APIServer pid=65602)[0;0m   "merge_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "min_frames": 4,
[0;36m(APIServer pid=65602)[0;0m   "num_frames": null,
[0;36m(APIServer pid=65602)[0;0m   "pad_size": null,
[0;36m(APIServer pid=65602)[0;0m   "patch_size": 16,
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor",
[0;36m(APIServer pid=65602)[0;0m   "resample": 3,
[0;36m(APIServer pid=65602)[0;0m   "rescale_factor": 0.00392156862745098,
[0;36m(APIServer pid=65602)[0;0m   "return_metadata": false,
[0;36m(APIServer pid=65602)[0;0m   "size": {
[0;36m(APIServer pid=65602)[0;0m     "longest_edge": 1048576,
[0;36m(APIServer pid=65602)[0;0m     "shortest_edge": 65536
[0;36m(APIServer pid=65602)[0;0m   },
[0;36m(APIServer pid=65602)[0;0m   "temporal_patch_size": 2,
[0;36m(APIServer pid=65602)[0;0m   "video_metadata": null,
[0;36m(APIServer pid=65602)[0;0m   "video_processor_type": "Qwen3VLVideoProcessor"
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m {
[0;36m(APIServer pid=65602)[0;0m   "processor_class": "Qwen3VLProcessor"
[0;36m(APIServer pid=65602)[0;0m }
[0;36m(APIServer pid=65602)[0;0m 
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:57:12 [v1/engine/core.py:988] EngineCore loop active.
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:57:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40606 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:57:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 1234.6 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 4 reqs, Waiting: 4 reqs, GPU KV cache usage: 29.9%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:57:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 2469.4 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 7 reqs, Waiting: 1 reqs, GPU KV cache usage: 51.5%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:57:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 6173.4 tokens/s, Avg generation throughput: 103.0 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 66.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:57:52 [v1/engine/core.py:982] EngineCore waiting for work.
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:57:52 [v1/engine/core.py:988] EngineCore loop active.
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40606 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:57:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 196.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:58:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 1234.7 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 4 reqs, Waiting: 2 reqs, GPU KV cache usage: 29.9%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:58:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 6173.4 tokens/s, Avg generation throughput: 0.9 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 65.3%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:58:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 2469.6 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 66.7%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:58:28 [v1/engine/core.py:982] EngineCore waiting for work.
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:58:28 [v1/engine/core.py:988] EngineCore loop active.
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:58:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 1234.6 tokens/s, Avg generation throughput: 97.5 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.5%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:58:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 3704.1 tokens/s, Avg generation throughput: 107.5 tokens/s, Running: 4 reqs, Waiting: 0 reqs, GPU KV cache usage: 33.3%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(EngineCore_DP0 pid=65986)[0;0m DEBUG 02-09 19:58:47 [v1/engine/core.py:982] EngineCore waiting for work.
[0;36m(APIServer pid=65602)[0;0m INFO:     127.0.0.1:40596 - "GET /metrics HTTP/1.1" 200 OK
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:58:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 44.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m INFO 02-09 19:59:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:59:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:59:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:59:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:59:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 19:59:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:00:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:00:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:00:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:00:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:00:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:00:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:01:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:01:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:01:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:01:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:01:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:01:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:02:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:02:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:02:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:02:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:02:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:02:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:03:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:03:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:03:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:03:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:03:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:03:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:04:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:04:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:04:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:04:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:04:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:04:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:05:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:05:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:05:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:05:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:05:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:05:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:06:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:06:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:06:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:06:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:06:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:06:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:07:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:07:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:07:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:07:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:07:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:07:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:08:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:08:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:08:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:08:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:08:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:08:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:09:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:09:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:09:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:09:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:09:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:09:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:10:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:10:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:10:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:10:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:10:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:10:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:11:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:11:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:11:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:11:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:11:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:11:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:12:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:12:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:12:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:12:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:12:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:12:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:13:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:13:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:13:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:13:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:13:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:13:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:14:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:14:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:14:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:14:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:14:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:14:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:15:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:15:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:15:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:15:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:15:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:15:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:16:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:16:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:16:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:16:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:16:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:16:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:17:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:17:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:17:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:17:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:17:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:17:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:18:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:18:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:18:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:18:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:18:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:18:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:19:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:19:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:19:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:19:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:19:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:19:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:20:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:20:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:20:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:20:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:20:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:20:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:21:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:21:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:21:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:21:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:21:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:21:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:22:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:22:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:22:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:22:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:22:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:22:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:23:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:23:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:23:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:23:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:23:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:23:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:24:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:24:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:24:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:24:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:24:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:24:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:25:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:25:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:25:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:25:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:25:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:25:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:26:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:26:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:26:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:26:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:26:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:26:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:27:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:27:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:27:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:27:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:27:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:27:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:28:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:28:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:28:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:28:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:28:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:28:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:29:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:29:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:29:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:29:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:29:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:29:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:30:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:30:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:30:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:30:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:30:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:30:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:31:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:31:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:31:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:31:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:31:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:31:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:32:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:32:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:32:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:32:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:32:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:32:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:33:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:33:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:33:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:33:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:33:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:33:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:34:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:34:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:34:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:34:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:34:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:34:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:35:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:35:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:35:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:35:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:35:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:35:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:36:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:36:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:36:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:36:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:36:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:36:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:37:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:37:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:37:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:37:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:37:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:37:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:38:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:38:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:38:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:38:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:38:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:38:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:39:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:39:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:39:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:39:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:39:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:39:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:40:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:40:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:40:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:40:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:40:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:40:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:41:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:41:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:41:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:41:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:41:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:41:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:42:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:42:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:42:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:42:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:42:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:42:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:43:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:43:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:43:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:43:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:43:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:43:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:44:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:44:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:44:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:44:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:44:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:44:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:45:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:45:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:45:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:45:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:45:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:45:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:46:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:46:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:46:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:46:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:46:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:46:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:47:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:47:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:47:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:47:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:47:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:47:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:48:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:48:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:48:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:48:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:48:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:48:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:49:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:49:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:49:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:49:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:49:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:49:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:50:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:50:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:50:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:50:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:50:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:50:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:51:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:51:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:51:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:51:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:51:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:51:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:52:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:52:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:52:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:52:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:52:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:52:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:53:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:53:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:53:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:53:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:53:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:53:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:54:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:54:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:54:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:54:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:54:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:54:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:55:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:55:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:55:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:55:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:55:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:55:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:56:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:56:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:56:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:56:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:56:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:56:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:57:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:57:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:57:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:57:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:57:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:57:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:58:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:58:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:58:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:58:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:58:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:58:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:59:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:59:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:59:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:59:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:59:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 20:59:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:00:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:00:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:00:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:00:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:00:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:00:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:01:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:01:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:01:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:01:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:01:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:01:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:02:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:02:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:02:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:02:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:02:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:02:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:03:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:03:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:03:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:03:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:03:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:03:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:04:04 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:04:14 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:04:24 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:04:34 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:04:44 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:04:54 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:05:05 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:05:15 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:05:25 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:05:35 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:05:45 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:05:55 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:06:05 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:06:15 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:06:25 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:06:35 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:06:45 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:06:55 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:07:05 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:07:15 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:07:25 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:07:35 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:07:45 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:07:55 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:08:05 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:08:15 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:08:25 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:08:35 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:08:45 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:08:55 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:09:05 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:09:15 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:09:25 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:09:35 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:09:45 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:09:55 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:10:05 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:10:15 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:10:25 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:10:35 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:10:45 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:10:55 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:11:05 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:11:15 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:11:25 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:11:35 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:11:45 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:11:55 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:12:05 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:12:15 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:12:25 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:12:35 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:12:45 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:12:55 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:13:05 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:13:15 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:13:25 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:13:35 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:13:45 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:13:55 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:14:05 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:14:15 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:14:25 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:14:35 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:14:45 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:14:55 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:15:05 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:15:15 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:15:25 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:15:35 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:15:45 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:15:55 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:16:05 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:16:15 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:16:25 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:16:35 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:16:45 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:16:55 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:17:05 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:17:15 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:17:25 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:17:35 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:17:45 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:17:55 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:18:05 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:18:15 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:18:25 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:18:35 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[0;36m(APIServer pid=65602)[0;0m DEBUG 02-09 21:18:45 [v1/metrics/loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
