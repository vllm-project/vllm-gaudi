vllm bench serve   --backend openai-chat --base-url http://localhost:12346 --endpoint /v1/chat/completions \
  --model  /root/.cache/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct-FP8/snapshots/4bf2c2f39c37c0fede78bede4056e1f18cdf8109/ \
  --num-prompts 20  --random-input-len 6300  --random-output-len 380 --random-mm-base-items-per-request 20  --random-mm-num-mm-items-range-ratio 0.0  --max-concurrency 8   --percentile-metrics ttft,tpot,itl,e2el   --metric-percentiles 50,90,99  --ignore-eos   --trust-remote-code 2>&1 | tee blog.txt